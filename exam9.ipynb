{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k87tspUobsTm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "FLEXIBLE MULTI-CLASS CLASSIFICATION PIPELINE\n",
        "================================================================================\n",
        "A production-ready, competition-focused ML pipeline that automatically handles:\n",
        "- Single/Multi-target classification\n",
        "- Probability predictions for competitions (Log Loss)\n",
        "- Any number of classes\n",
        "- Various data distributions\n",
        "- Minimal configuration needed\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import (roc_auc_score, log_loss, classification_report,\n",
        "                            confusion_matrix, accuracy_score, f1_score)\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. USER CONFIGURATION - MODIFY THIS SECTION ONLY\n",
        "# ============================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    # File paths\n",
        "    'train_path': \"/kaggle/input/mle-ese-mock/train (5).csv\",\n",
        "    'test_path': \"/kaggle/input/mle-ese-mock/test (4).csv\",\n",
        "    'sample_submission_path': \"/kaggle/input/mle-ese-mock/sample_submission.csv\",  # Path to sample submission\n",
        "    'output_file': \"submission.csv\",\n",
        "\n",
        "    # Column configuration\n",
        "    'target_col': \"quality_grade\",  # Can be string or list for multi-target\n",
        "    'id_col': \"id\",\n",
        "\n",
        "    # Competition type\n",
        "    'submission_type': 'probabilities',  # 'probabilities' or 'classes'\n",
        "    # For probability submissions, set to 'probabilities' (used for Log Loss)\n",
        "    # For class label submissions, set to 'classes'\n",
        "\n",
        "    # Column prefix for submission (e.g., 'Status_' will create 'Status_Q1_premium_fresh')\n",
        "    'submission_column_prefix': 'Status_',  # Set to '' or None for no prefix\n",
        "\n",
        "    # Model configuration\n",
        "    'model_type': 'random_forest',  # 'random_forest' or 'gradient_boosting'\n",
        "    'n_estimators': 1000,\n",
        "    'use_class_weight': True,  # Auto-handle imbalanced classes\n",
        "\n",
        "    # Processing options\n",
        "    'do_plotting': True,\n",
        "    'do_outlier_cap': False,\n",
        "    'do_hyperparam_tuning': True,\n",
        "    'use_cross_validation': False,  # Use CV for more robust evaluation\n",
        "\n",
        "    # Advanced options\n",
        "    'validation_size': 0.2,\n",
        "    'random_state': 42,\n",
        "    'cv_folds': 5,\n",
        "    'n_iter_search': 20,\n",
        "}\n",
        "\n",
        "RANDOM_STATE = CONFIG['random_state']\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. HELPER FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def print_section(title):\n",
        "    \"\"\"Print a formatted section header\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(title.upper())\n",
        "    print(\"=\"*80)\n",
        "\n",
        "def print_subsection(title):\n",
        "    \"\"\"Print a formatted subsection header\"\"\"\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(title)\n",
        "    print(\"-\"*80)\n",
        "\n",
        "def detect_feature_types(df, exclude_cols=None):\n",
        "    \"\"\"Automatically detect categorical and numerical columns\"\"\"\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = []\n",
        "\n",
        "    df = df.drop(columns=exclude_cols, errors='ignore')\n",
        "\n",
        "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "    # Additional check: if numerical column has few unique values, treat as categorical\n",
        "    for col in num_cols.copy():\n",
        "        if df[col].nunique() < 10 and df[col].nunique() < len(df) * 0.05:\n",
        "            print(f\"  â„¹ '{col}' has few unique values ({df[col].nunique()}), treating as categorical\")\n",
        "            cat_cols.append(col)\n",
        "            num_cols.remove(col)\n",
        "\n",
        "    return cat_cols, num_cols\n",
        "\n",
        "def safe_roc_auc(y_true, y_pred_proba, multi_class='ovr'):\n",
        "    \"\"\"Safely compute ROC AUC with error handling\"\"\"\n",
        "    try:\n",
        "        if len(np.unique(y_true)) == 2:\n",
        "            return roc_auc_score(y_true, y_pred_proba[:, 1])\n",
        "        else:\n",
        "            return roc_auc_score(y_true, y_pred_proba,\n",
        "                               multi_class=multi_class, average='macro')\n",
        "    except Exception as e:\n",
        "        print(f\"  âš  Could not compute ROC AUC: {e}\")\n",
        "        return None\n",
        "\n",
        "def plot_target_distribution(data, target_cols, figsize_per_plot=6):\n",
        "    \"\"\"Plot distribution of target variable(s)\"\"\"\n",
        "    n_targets = len(target_cols) if isinstance(target_cols, list) else 1\n",
        "    target_cols = [target_cols] if isinstance(target_cols, str) else target_cols\n",
        "\n",
        "    fig, axes = plt.subplots(1, n_targets, figsize=(figsize_per_plot*n_targets, 5))\n",
        "    if n_targets == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, target_col in enumerate(target_cols):\n",
        "        counts = data[target_col].value_counts().sort_index()\n",
        "        axes[idx].bar(range(len(counts)), counts.values)\n",
        "        axes[idx].set_title(f\"Distribution: '{target_col}'\")\n",
        "        axes[idx].set_xlabel(\"Class\")\n",
        "        axes[idx].set_ylabel(\"Count\")\n",
        "        axes[idx].set_xticks(range(len(counts)))\n",
        "        axes[idx].set_xticklabels(counts.index, rotation=45, ha='right')\n",
        "\n",
        "        # Add count labels on bars\n",
        "        for i, v in enumerate(counts.values):\n",
        "            axes[idx].text(i, v, str(v), ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DATA LOADING AND INITIAL INSPECTION\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"DATA LOADING AND INITIAL INSPECTION\")\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv(CONFIG['train_path'])\n",
        "test_data = pd.read_csv(CONFIG['test_path'])\n",
        "\n",
        "print(f\"\\nâœ“ Data loaded successfully\")\n",
        "print(f\"  Train shape: {train_data.shape}\")\n",
        "print(f\"  Test shape: {test_data.shape}\")\n",
        "\n",
        "# Load sample submission to understand expected format\n",
        "sample_submission = None\n",
        "if CONFIG.get('sample_submission_path'):\n",
        "    try:\n",
        "        sample_submission = pd.read_csv(CONFIG['sample_submission_path'])\n",
        "        print(f\"\\nâœ“ Sample submission loaded\")\n",
        "        print(f\"  Shape: {sample_submission.shape}\")\n",
        "        print(f\"  Columns: {list(sample_submission.columns)}\")\n",
        "        print(f\"\\nSample submission preview:\")\n",
        "        print(sample_submission.head(3))\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš  Could not load sample submission: {e}\")\n",
        "        print(\"  Will use default format\")\n",
        "\n",
        "# Save test IDs\n",
        "test_ids = test_data[CONFIG['id_col']].copy() if CONFIG['id_col'] in test_data.columns else None\n",
        "\n",
        "# Drop ID columns\n",
        "if CONFIG['id_col'] in train_data.columns:\n",
        "    train_data = train_data.drop(columns=[CONFIG['id_col']])\n",
        "if CONFIG['id_col'] in test_data.columns:\n",
        "    test_data = test_data.drop(columns=[CONFIG['id_col']])\n",
        "\n",
        "# Display basic info\n",
        "print(\"\\nTrain data sample:\")\n",
        "print(train_data.head(3))\n",
        "print(\"\\nTrain data info:\")\n",
        "print(train_data.info())\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. TARGET CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"TARGET CONFIGURATION\")\n",
        "\n",
        "# Normalize target_col to list\n",
        "if isinstance(CONFIG['target_col'], str):\n",
        "    target_cols = [CONFIG['target_col']]\n",
        "    is_multi_target = False\n",
        "    print(f\"\\nâœ“ Single target mode: '{CONFIG['target_col']}'\")\n",
        "elif isinstance(CONFIG['target_col'], list):\n",
        "    target_cols = CONFIG['target_col']\n",
        "    is_multi_target = True\n",
        "    print(f\"\\nâœ“ Multi-target mode: {len(target_cols)} targets\")\n",
        "    print(f\"  Targets: {target_cols}\")\n",
        "else:\n",
        "    raise ValueError(\"TARGET_COL must be a string or list of strings\")\n",
        "\n",
        "# Verify targets exist\n",
        "missing_targets = [col for col in target_cols if col not in train_data.columns]\n",
        "if missing_targets:\n",
        "    raise ValueError(f\"Target columns not found: {missing_targets}\")\n",
        "\n",
        "# Analyze each target\n",
        "target_info = {}\n",
        "for target_col in target_cols:\n",
        "    print_subsection(f\"Target: {target_col}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    nan_count = train_data[target_col].isnull().sum()\n",
        "    if nan_count > 0:\n",
        "        print(f\"  âš  WARNING: {nan_count} missing values in target\")\n",
        "\n",
        "    # Get class information\n",
        "    unique_classes = sorted(train_data[target_col].dropna().unique())\n",
        "    n_classes = len(unique_classes)\n",
        "\n",
        "    print(f\"  Number of classes: {n_classes}\")\n",
        "    print(f\"  Classes: {unique_classes}\")\n",
        "\n",
        "    # Class distribution\n",
        "    value_counts = train_data[target_col].value_counts()\n",
        "    print(\"\\n  Class distribution:\")\n",
        "    for cls in unique_classes:\n",
        "        count = value_counts.get(cls, 0)\n",
        "        pct = (count / len(train_data)) * 100\n",
        "        print(f\"    {cls}: {count:>6} ({pct:>5.2f}%)\")\n",
        "\n",
        "    # Check for imbalance\n",
        "    min_pct = (value_counts.min() / len(train_data)) * 100\n",
        "    if min_pct < 5:\n",
        "        print(f\"\\n  âš  Severe class imbalance detected! Minimum class: {min_pct:.2f}%\")\n",
        "        if not CONFIG['use_class_weight']:\n",
        "            print(\"  ðŸ’¡ Consider setting 'use_class_weight': True\")\n",
        "\n",
        "    target_info[target_col] = {\n",
        "        'n_classes': n_classes,\n",
        "        'classes': unique_classes,\n",
        "        'nan_count': nan_count\n",
        "    }\n",
        "\n",
        "# Plot distributions\n",
        "if CONFIG['do_plotting']:\n",
        "    plot_target_distribution(train_data, target_cols)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. DATA CLEANING\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"DATA CLEANING\")\n",
        "\n",
        "# Remove duplicates\n",
        "train_dups = train_data.duplicated().sum()\n",
        "if train_dups > 0:\n",
        "    print(f\"âœ“ Removing {train_dups} duplicate rows\")\n",
        "    train_data = train_data.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Missing values analysis\n",
        "print_subsection(\"Missing Values Analysis\")\n",
        "\n",
        "train_missing = train_data.isnull().sum()\n",
        "test_missing = test_data.isnull().sum()\n",
        "\n",
        "print(\"\\nTrain missing values:\")\n",
        "if train_missing.sum() == 0:\n",
        "    print(\"  âœ“ No missing values\")\n",
        "else:\n",
        "    missing_cols = train_missing[train_missing > 0]\n",
        "    for col, count in missing_cols.items():\n",
        "        pct = (count / len(train_data)) * 100\n",
        "        print(f\"  {col}: {count} ({pct:.2f}%)\")\n",
        "\n",
        "print(\"\\nTest missing values:\")\n",
        "if test_missing.sum() == 0:\n",
        "    print(\"  âœ“ No missing values\")\n",
        "else:\n",
        "    missing_cols = test_missing[test_missing > 0]\n",
        "    for col, count in missing_cols.items():\n",
        "        pct = (count / len(test_data)) * 100\n",
        "        print(f\"  {col}: {count} ({pct:.2f}%)\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. FEATURE EXTRACTION AND SEPARATION\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"FEATURE EXTRACTION\")\n",
        "\n",
        "# Separate features and targets\n",
        "X = train_data.drop(columns=target_cols)\n",
        "y = train_data[target_cols]\n",
        "\n",
        "# Handle missing targets\n",
        "total_nan = y.isnull().any(axis=1).sum()\n",
        "if total_nan > 0:\n",
        "    print(f\"\\nâš  Removing {total_nan} rows with missing target values\")\n",
        "    valid_idx = ~y.isnull().any(axis=1)\n",
        "    X = X[valid_idx].reset_index(drop=True)\n",
        "    y = y[valid_idx].reset_index(drop=True)\n",
        "\n",
        "# Convert to Series for single target\n",
        "if not is_multi_target:\n",
        "    y = y.iloc[:, 0]\n",
        "\n",
        "# Detect feature types\n",
        "cat_cols, num_cols = detect_feature_types(X)\n",
        "\n",
        "print(f\"\\nâœ“ Feature summary:\")\n",
        "print(f\"  Total features: {X.shape[1]}\")\n",
        "print(f\"  Categorical: {len(cat_cols)}\")\n",
        "print(f\"  Numerical: {len(num_cols)}\")\n",
        "\n",
        "if cat_cols:\n",
        "    print(f\"\\n  Categorical columns: {cat_cols}\")\n",
        "if num_cols:\n",
        "    print(f\"\\n  Numerical columns: {num_cols}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 7. EXPLORATORY DATA ANALYSIS (Optional)\n",
        "# ============================================================================\n",
        "\n",
        "if CONFIG['do_plotting']:\n",
        "    print_section(\"EXPLORATORY DATA ANALYSIS\")\n",
        "\n",
        "    # Numerical features - distributions\n",
        "    if len(num_cols) > 0:\n",
        "        print(\"\\nâœ“ Plotting numerical feature distributions...\")\n",
        "        n_num = min(len(num_cols), 12)  # Limit to 12 plots\n",
        "        ncols = 3\n",
        "        nrows = (n_num + ncols - 1) // ncols\n",
        "\n",
        "        fig, axes = plt.subplots(nrows, ncols, figsize=(15, 4*nrows))\n",
        "        axes = axes.flatten() if n_num > 1 else [axes]\n",
        "\n",
        "        for i, col in enumerate(num_cols[:n_num]):\n",
        "            sns.histplot(X[col].dropna(), kde=True, ax=axes[i])\n",
        "            axes[i].set_title(col, fontsize=10)\n",
        "\n",
        "        for i in range(n_num, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Categorical features - top categories\n",
        "    if len(cat_cols) > 0:\n",
        "        print(\"\\nâœ“ Plotting categorical feature distributions...\")\n",
        "        for col in cat_cols[:5]:  # Limit to 5\n",
        "            plt.figure(figsize=(10, 4))\n",
        "            top_cats = X[col].value_counts().head(15)\n",
        "            sns.barplot(x=top_cats.values, y=top_cats.index)\n",
        "            plt.title(f\"Top Categories: {col}\")\n",
        "            plt.xlabel(\"Count\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 8. PREPROCESSING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"DATA PREPROCESSING\")\n",
        "\n",
        "# Imputation\n",
        "print(\"\\nâœ“ Setting up imputation...\")\n",
        "if num_cols:\n",
        "    num_imputer = SimpleImputer(strategy='median')\n",
        "    X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
        "    test_data[num_cols] = num_imputer.transform(test_data[num_cols])\n",
        "    print(\"  - Numerical: median imputation\")\n",
        "\n",
        "if cat_cols:\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
        "    test_data[cat_cols] = cat_imputer.transform(test_data[cat_cols])\n",
        "    print(\"  - Categorical: most frequent imputation\")\n",
        "\n",
        "# Outlier capping (optional)\n",
        "if CONFIG['do_outlier_cap'] and num_cols:\n",
        "    print(\"\\nâœ“ Capping outliers (IQR method)...\")\n",
        "    for col in num_cols:\n",
        "        q1 = X[col].quantile(0.25)\n",
        "        q3 = X[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower = q1 - 1.5 * iqr\n",
        "        upper = q3 + 1.5 * iqr\n",
        "\n",
        "        X[col] = X[col].clip(lower=lower, upper=upper)\n",
        "        test_data[col] = test_data[col].clip(lower=lower, upper=upper)\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "print(\"\\nâœ“ Building preprocessing pipeline...\")\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
        "        ('num', StandardScaler(), num_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 9. TRAIN/VALIDATION SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"TRAIN/VALIDATION SPLIT\")\n",
        "\n",
        "# Stratify for single-target only\n",
        "stratify_param = y if not is_multi_target else None\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=CONFIG['validation_size'],\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=stratify_param\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Split completed:\")\n",
        "print(f\"  Train: {X_train.shape[0]} samples\")\n",
        "print(f\"  Validation: {X_val.shape[0]} samples\")\n",
        "\n",
        "# Transform features\n",
        "print(\"\\nâœ“ Transforming features...\")\n",
        "X_train_pre = preprocessor.fit_transform(X_train)\n",
        "X_val_pre = preprocessor.transform(X_val)\n",
        "test_data_pre = preprocessor.transform(test_data)\n",
        "\n",
        "print(f\"  Train shape: {X_train_pre.shape}\")\n",
        "print(f\"  Val shape: {X_val_pre.shape}\")\n",
        "print(f\"  Test shape: {test_data_pre.shape}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 10. LABEL ENCODING\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"LABEL ENCODING\")\n",
        "\n",
        "if not is_multi_target:\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(y_train)\n",
        "    y_val_enc = le.transform(y_val)\n",
        "\n",
        "    print(f\"\\nâœ“ Encoded {len(le.classes_)} classes\")\n",
        "    print(f\"  Classes: {le.classes_}\")\n",
        "else:\n",
        "    label_encoders = {}\n",
        "    y_train_enc = pd.DataFrame(index=y_train.index)\n",
        "    y_val_enc = pd.DataFrame(index=y_val.index)\n",
        "\n",
        "    for target_col in target_cols:\n",
        "        le_temp = LabelEncoder()\n",
        "        y_train_enc[target_col] = le_temp.fit_transform(y_train[target_col])\n",
        "        y_val_enc[target_col] = le_temp.transform(y_val[target_col])\n",
        "        label_encoders[target_col] = le_temp\n",
        "\n",
        "        print(f\"\\nâœ“ Target '{target_col}': {len(le_temp.classes_)} classes\")\n",
        "\n",
        "    y_train_enc = y_train_enc.values\n",
        "    y_val_enc = y_val_enc.values\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 11. MODEL TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"MODEL TRAINING\")\n",
        "\n",
        "# Select base model\n",
        "if CONFIG['model_type'] == 'random_forest':\n",
        "    print(\"\\nâœ“ Using Random Forest Classifier\")\n",
        "    base_model = RandomForestClassifier(\n",
        "        n_estimators=CONFIG['n_estimators'],\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight='balanced' if CONFIG['use_class_weight'] else None,\n",
        "        n_jobs=-1,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5\n",
        "    )\n",
        "elif CONFIG['model_type'] == 'gradient_boosting':\n",
        "    print(\"\\nâœ“ Using Gradient Boosting Classifier\")\n",
        "    base_model = GradientBoostingClassifier(\n",
        "        n_estimators=CONFIG['n_estimators'],\n",
        "        random_state=RANDOM_STATE,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f\"Unknown model type: {CONFIG['model_type']}\")\n",
        "\n",
        "# Wrap for multi-target if needed\n",
        "if is_multi_target:\n",
        "    print(\"  (Multi-Output wrapper enabled)\")\n",
        "    model = MultiOutputClassifier(base_model, n_jobs=-1)\n",
        "else:\n",
        "    model = base_model\n",
        "\n",
        "# Train\n",
        "print(\"\\nâ³ Training model...\")\n",
        "model.fit(X_train_pre, y_train_enc)\n",
        "print(\"âœ“ Training completed\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 12. MODEL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"MODEL EVALUATION\")\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = model.predict(X_train_pre)\n",
        "y_val_pred = model.predict(X_val_pre)\n",
        "\n",
        "if is_multi_target:\n",
        "    print(\"\\nðŸ“Š Multi-Target Metrics:\")\n",
        "    print(\"\\nValidation Set:\")\n",
        "    for idx, target_col in enumerate(target_cols):\n",
        "        acc = accuracy_score(y_val_enc[:, idx], y_val_pred[:, idx])\n",
        "        f1 = f1_score(y_val_enc[:, idx], y_val_pred[:, idx], average='macro')\n",
        "        print(f\"  {target_col}:\")\n",
        "        print(f\"    Accuracy: {acc:.4f}\")\n",
        "        print(f\"    F1 Score: {f1:.4f}\")\n",
        "else:\n",
        "    # Get probabilities\n",
        "    try:\n",
        "        y_train_proba = model.predict_proba(X_train_pre)\n",
        "        y_val_proba = model.predict_proba(X_val_pre)\n",
        "\n",
        "        # Calculate metrics\n",
        "        roc_train = safe_roc_auc(y_train_enc, y_train_proba)\n",
        "        roc_val = safe_roc_auc(y_val_enc, y_val_proba)\n",
        "        loss_train = log_loss(y_train_enc, y_train_proba)\n",
        "        loss_val = log_loss(y_val_enc, y_val_proba)\n",
        "\n",
        "        print(\"\\nðŸ“Š Training Metrics:\")\n",
        "        if roc_train:\n",
        "            print(f\"  ROC AUC: {roc_train:.4f}\")\n",
        "        print(f\"  Log Loss: {loss_train:.4f}\")\n",
        "\n",
        "        print(\"\\nðŸ“Š Validation Metrics:\")\n",
        "        if roc_val:\n",
        "            print(f\"  ROC AUC: {roc_val:.4f}\")\n",
        "        print(f\"  Log Loss: {loss_val:.4f}\")\n",
        "\n",
        "        # Accuracy and F1\n",
        "        acc_val = accuracy_score(y_val_enc, y_val_pred)\n",
        "        f1_val = f1_score(y_val_enc, y_val_pred, average='macro')\n",
        "        print(f\"  Accuracy: {acc_val:.4f}\")\n",
        "        print(f\"  F1 Score (Macro): {f1_val:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš  Error computing probability metrics: {e}\")\n",
        "\n",
        "# Classification report\n",
        "if not is_multi_target:\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"Classification Report (Validation):\")\n",
        "    print(\"-\"*80)\n",
        "    print(classification_report(y_val_enc, y_val_pred,\n",
        "                               target_names=[str(c) for c in le.classes_],\n",
        "                               zero_division=0))\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 13. HYPERPARAMETER TUNING (Optional)\n",
        "# ============================================================================\n",
        "\n",
        "tuned_model = None\n",
        "if CONFIG['do_hyperparam_tuning']:\n",
        "    print_section(\"HYPERPARAMETER TUNING\")\n",
        "\n",
        "    print(\"\\nâ³ Starting RandomizedSearchCV...\")\n",
        "\n",
        "    # Define parameter distributions\n",
        "    if CONFIG['model_type'] == 'random_forest':\n",
        "        param_prefix = \"estimator__\" if is_multi_target else \"\"\n",
        "        param_dist = {\n",
        "            f\"{param_prefix}n_estimators\": [200, 500, 800, 1000],\n",
        "            f\"{param_prefix}max_depth\": [None, 10, 15, 20, 25],\n",
        "            f\"{param_prefix}min_samples_split\": [2, 5, 10],\n",
        "            f\"{param_prefix}min_samples_leaf\": [1, 2, 4],\n",
        "            f\"{param_prefix}max_features\": [\"sqrt\", \"log2\", 0.3]\n",
        "        }\n",
        "    else:\n",
        "        param_prefix = \"estimator__\" if is_multi_target else \"\"\n",
        "        param_dist = {\n",
        "            f\"{param_prefix}n_estimators\": [100, 200, 300],\n",
        "            f\"{param_prefix}max_depth\": [3, 5, 7],\n",
        "            f\"{param_prefix}learning_rate\": [0.01, 0.05, 0.1],\n",
        "            f\"{param_prefix}min_samples_split\": [2, 5, 10]\n",
        "        }\n",
        "\n",
        "    # Choose scoring\n",
        "    if is_multi_target:\n",
        "        scoring = 'accuracy'\n",
        "    else:\n",
        "        n_classes = len(le.classes_)\n",
        "        scoring = 'roc_auc_ovr' if n_classes > 2 else 'roc_auc'\n",
        "\n",
        "    # Run search\n",
        "    rnd_search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=CONFIG['n_iter_search'],\n",
        "        scoring=scoring,\n",
        "        cv=3,\n",
        "        verbose=1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    rnd_search.fit(X_train_pre, y_train_enc)\n",
        "\n",
        "    print(\"\\nâœ“ Best parameters:\")\n",
        "    for param, value in rnd_search.best_params_.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "    print(f\"\\nBest CV score: {rnd_search.best_score_:.4f}\")\n",
        "\n",
        "    tuned_model = rnd_search.best_estimator_\n",
        "\n",
        "    # Evaluate tuned model\n",
        "    y_val_pred_tuned = tuned_model.predict(X_val_pre)\n",
        "\n",
        "    if not is_multi_target:\n",
        "        try:\n",
        "            y_val_proba_tuned = tuned_model.predict_proba(X_val_pre)\n",
        "            loss_val_tuned = log_loss(y_val_enc, y_val_proba_tuned)\n",
        "            acc_val_tuned = accuracy_score(y_val_enc, y_val_pred_tuned)\n",
        "\n",
        "            print(f\"\\nðŸ“Š Tuned Model - Validation Metrics:\")\n",
        "            print(f\"  Log Loss: {loss_val_tuned:.4f}\")\n",
        "            print(f\"  Accuracy: {acc_val_tuned:.4f}\")\n",
        "\n",
        "            if loss_val_tuned < loss_val:\n",
        "                print(\"  âœ“ Improvement over baseline!\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 14. GENERATE PREDICTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"GENERATING PREDICTIONS\")\n",
        "\n",
        "final_model = tuned_model if tuned_model is not None else model\n",
        "print(f\"\\nâœ“ Using {'TUNED' if tuned_model else 'BASELINE'} model\")\n",
        "\n",
        "# Determine submission format\n",
        "if CONFIG['submission_type'] == 'probabilities':\n",
        "    print(\"âœ“ Generating probability predictions (for Log Loss)\")\n",
        "\n",
        "    if is_multi_target:\n",
        "        # Multi-target probabilities not commonly used, generate classes\n",
        "        print(\"  â„¹ Multi-target with probabilities not standard, using class predictions\")\n",
        "        test_pred = final_model.predict(test_data_pre)\n",
        "\n",
        "        submission_df = pd.DataFrame({CONFIG['id_col']: test_ids})\n",
        "        for idx, target_col in enumerate(target_cols):\n",
        "            le_temp = label_encoders[target_col]\n",
        "            submission_df[target_col] = le_temp.inverse_transform(test_pred[:, idx])\n",
        "    else:\n",
        "        # Generate probabilities for each class\n",
        "        test_proba = final_model.predict_proba(test_data_pre)\n",
        "\n",
        "        # Create submission DataFrame\n",
        "        if test_ids is not None:\n",
        "            submission_df = pd.DataFrame({CONFIG['id_col']: test_ids})\n",
        "        else:\n",
        "            submission_df = pd.DataFrame()\n",
        "\n",
        "        # Determine column order from sample submission if available\n",
        "        if sample_submission is not None:\n",
        "            # Get probability column names from sample submission (exclude ID column)\n",
        "            expected_columns = [col for col in sample_submission.columns if col != CONFIG['id_col']]\n",
        "\n",
        "            print(f\"\\n  Using column order from sample submission:\")\n",
        "            print(f\"  Expected columns: {expected_columns}\")\n",
        "\n",
        "            # Use the exact columns from sample submission\n",
        "            class_to_idx = {class_name: idx for idx, class_name in enumerate(le.classes_)}\n",
        "\n",
        "            # Create variations to handle prefixes\n",
        "            class_name_variations = {}\n",
        "            for class_name in le.classes_:\n",
        "                class_name_variations[class_name] = class_name\n",
        "                for prefix in ['Status_', 'status_', 'TARGET_', 'target_', 'Class_', 'class_']:\n",
        "                    if class_name.startswith(prefix):\n",
        "                        clean_name = class_name[len(prefix):]\n",
        "                        class_name_variations[clean_name] = class_name\n",
        "                    prefixed = prefix + class_name\n",
        "                    class_name_variations[prefixed] = class_name\n",
        "\n",
        "            # Add columns in exact order from sample submission\n",
        "            columns_added = 0\n",
        "            for col_name in expected_columns:\n",
        "                matched_class = None\n",
        "\n",
        "                if col_name in class_to_idx:\n",
        "                    matched_class = col_name\n",
        "                elif col_name in class_name_variations:\n",
        "                    matched_class = class_name_variations[col_name]\n",
        "                else:\n",
        "                    for prefix in ['Status_', 'status_', 'TARGET_', 'target_', 'Class_', 'class_']:\n",
        "                        if col_name.startswith(prefix):\n",
        "                            potential_match = col_name[len(prefix):]\n",
        "                            if potential_match in class_to_idx:\n",
        "                                matched_class = potential_match\n",
        "                                break\n",
        "\n",
        "                    if matched_class is None:\n",
        "                        for class_name in le.classes_:\n",
        "                            if col_name.endswith(class_name):\n",
        "                                matched_class = class_name\n",
        "                                break\n",
        "\n",
        "                if matched_class and matched_class in class_to_idx:\n",
        "                    idx = class_to_idx[matched_class]\n",
        "                    submission_df[col_name] = test_proba[:, idx]\n",
        "                    columns_added += 1\n",
        "                else:\n",
        "                    print(f\"  âš  Warning: Column '{col_name}' could not be mapped\")\n",
        "                    submission_df[col_name] = 0.0\n",
        "\n",
        "            print(f\"  âœ“ Mapped {columns_added}/{len(expected_columns)} columns from sample submission\")\n",
        "\n",
        "        else:\n",
        "            # No sample submission - use prefix from config\n",
        "            print(f\"\\n  No sample submission - using config settings\")\n",
        "\n",
        "            prefix = CONFIG.get('submission_column_prefix', '')\n",
        "            if prefix:\n",
        "                print(f\"  Adding prefix '{prefix}' to column names\")\n",
        "\n",
        "            # Sort class names to ensure proper order (Q1, Q2, ..., Q10)\n",
        "            classes_sorted = sorted(le.classes_, key=lambda x: (\n",
        "                int(''.join(filter(str.isdigit, str(x)))) if any(c.isdigit() for c in str(x)) else 0,\n",
        "                str(x)\n",
        "            ))\n",
        "\n",
        "            # Create mapping from sorted classes to probability indices\n",
        "            class_to_idx = {class_name: idx for idx, class_name in enumerate(le.classes_)}\n",
        "\n",
        "            for class_name in classes_sorted:\n",
        "                idx = class_to_idx[class_name]\n",
        "                # Add prefix if specified\n",
        "                column_name = f\"{prefix}{class_name}\" if prefix else class_name\n",
        "                submission_df[column_name] = test_proba[:, idx]\n",
        "\n",
        "        print(f\"\\n  Generated probabilities for {len(le.classes_)} classes\")\n",
        "        print(f\"  Final column order: {list(submission_df.columns)}\")\n",
        "\n",
        "else:  # 'classes'\n",
        "    print(\"âœ“ Generating class predictions\")\n",
        "\n",
        "    test_pred = final_model.predict(test_data_pre)\n",
        "\n",
        "    if is_multi_target:\n",
        "        submission_df = pd.DataFrame({CONFIG['id_col']: test_ids})\n",
        "        for idx, target_col in enumerate(target_cols):\n",
        "            le_temp = label_encoders[target_col]\n",
        "            submission_df[target_col] = le_temp.inverse_transform(test_pred[:, idx])\n",
        "    else:\n",
        "        test_pred_decoded = le.inverse_transform(test_pred)\n",
        "\n",
        "        if test_ids is not None:\n",
        "            submission_df = pd.DataFrame({\n",
        "                CONFIG['id_col']: test_ids,\n",
        "                target_cols[0]: test_pred_decoded\n",
        "            })\n",
        "        else:\n",
        "            submission_df = pd.DataFrame({target_cols[0]: test_pred_decoded})\n",
        "\n",
        "# Save submission\n",
        "submission_df.to_csv(CONFIG['output_file'], index=False)\n",
        "print(f\"\\nâœ“ Submission saved to '{CONFIG['output_file']}'\")\n",
        "\n",
        "# Verify format matches sample submission\n",
        "if sample_submission is not None:\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"FORMAT VERIFICATION\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Check column match\n",
        "    expected_cols = list(sample_submission.columns)\n",
        "    actual_cols = list(submission_df.columns)\n",
        "\n",
        "    if expected_cols == actual_cols:\n",
        "        print(\"âœ… Column names and order MATCH sample submission perfectly!\")\n",
        "    else:\n",
        "        print(\"âš  Column differences detected:\")\n",
        "        print(f\"  Expected: {expected_cols}\")\n",
        "        print(f\"  Actual:   {actual_cols}\")\n",
        "\n",
        "        missing = set(expected_cols) - set(actual_cols)\n",
        "        extra = set(actual_cols) - set(expected_cols)\n",
        "        if missing:\n",
        "            print(f\"  Missing columns: {missing}\")\n",
        "        if extra:\n",
        "            print(f\"  Extra columns: {extra}\")\n",
        "\n",
        "    # Check shape\n",
        "    if len(submission_df) == len(sample_submission):\n",
        "        print(f\"âœ… Row count matches: {len(submission_df)} rows\")\n",
        "    else:\n",
        "        print(f\"âš  Row count mismatch:\")\n",
        "        print(f\"  Expected: {len(sample_submission)} rows\")\n",
        "        print(f\"  Actual:   {len(submission_df)} rows\")\n",
        "\n",
        "# Display statistics\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SUBMISSION SUMMARY\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Total rows: {len(submission_df)}\")\n",
        "print(f\"Columns: {list(submission_df.columns)}\")\n",
        "\n",
        "print(\"\\nFirst 10 rows:\")\n",
        "print(submission_df.head(10))\n",
        "\n",
        "if CONFIG['submission_type'] == 'probabilities' and not is_multi_target:\n",
        "    # Verify probabilities sum to 1\n",
        "    prob_cols = [c for c in submission_df.columns if c != CONFIG['id_col']]\n",
        "    prob_sum = submission_df[prob_cols].sum(axis=1)\n",
        "    print(f\"\\nProbability verification:\")\n",
        "    print(f\"  Min sum: {prob_sum.min():.6f}\")\n",
        "    print(f\"  Max sum: {prob_sum.max():.6f}\")\n",
        "    print(f\"  All sums â‰ˆ 1.0: {np.allclose(prob_sum, 1.0)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nðŸ’¡ Tips:\")\n",
        "print(f\"  - For Log Loss competitions, use submission_type='probabilities'\")\n",
        "print(f\"  - Check sample_submission.csv format for required columns\")\n",
        "print(f\"  - Consider ensemble methods for better performance\")\n",
        "print(f\"  - Use cross-validation for more robust evaluation\")"
      ]
    }
  ]
}