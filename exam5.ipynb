{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrZqVIYt1Utf"
      },
      "outputs": [],
      "source": [
        "# importing the useful libraries for the work\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import roc_auc_score, log_loss, classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. USER CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "TRAIN_PATH = \"/kaggle/input/mock-test-2-mse-2/train.csv\"\n",
        "TEST_PATH = \"/kaggle/input/mock-test-2-mse-2/test.csv\"\n",
        "\n",
        "# TARGET_COL can be a string (single target) or list of strings (multiple targets)\n",
        "TARGET_COL = \"Status\"  # Change to [\"Class1\", \"Class2\"] for multiple targets\n",
        "# Example: TARGET_COL = [\"target1\", \"target2\", \"target3\"]\n",
        "\n",
        "ID_COL = \"id\"\n",
        "OUTPUT_FILE = \"submission.csv\"\n",
        "\n",
        "# Toggle heavy steps\n",
        "DO_PLOTTING = True\n",
        "DO_OUTLIER_CAP = False\n",
        "DO_HYPERPARAM_TUNING = True\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_data = pd.read_csv(TRAIN_PATH)\n",
        "test_data = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(f\"\\nTrain shape: {train_data.shape}\")\n",
        "print(f\"Test shape: {test_data.shape}\")\n",
        "\n",
        "# Keep test IDs for submission\n",
        "test_ids = test_data[ID_COL].copy() if ID_COL in test_data.columns else None\n",
        "\n",
        "# Drop ID cols from feature tables\n",
        "if ID_COL in train_data.columns:\n",
        "    train_data = train_data.drop(columns=[ID_COL])\n",
        "if ID_COL in test_data.columns:\n",
        "    test_data = test_data.drop(columns=[ID_COL])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DETECT TARGET CONFIGURATION (Single vs Multiple Targets)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET CONFIGURATION DETECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Normalize TARGET_COL to always be a list\n",
        "if isinstance(TARGET_COL, str):\n",
        "    target_cols = [TARGET_COL]\n",
        "    is_multi_target = False\n",
        "    print(f\"\\n✓ Single target mode: '{TARGET_COL}'\")\n",
        "elif isinstance(TARGET_COL, list):\n",
        "    target_cols = TARGET_COL\n",
        "    is_multi_target = True\n",
        "    print(f\"\\n✓ Multi-target mode: {len(target_cols)} targets\")\n",
        "    print(f\"  Targets: {target_cols}\")\n",
        "else:\n",
        "    raise ValueError(\"TARGET_COL must be a string or list of strings\")\n",
        "\n",
        "# Verify all target columns exist\n",
        "missing_targets = [col for col in target_cols if col not in train_data.columns]\n",
        "if missing_targets:\n",
        "    raise ValueError(f\"Target columns not found in train data: {missing_targets}\")\n",
        "\n",
        "print(f\"\\nNumber of targets: {len(target_cols)}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. DATA CLEANING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA CLEANING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Basic info\n",
        "print(\"\\nTrain data info:\")\n",
        "print(train_data.info())\n",
        "print(\"\\nTest data info:\")\n",
        "print(test_data.info())\n",
        "\n",
        "# Duplicates\n",
        "train_dups = train_data.duplicated().sum()\n",
        "print(f\"\\nDuplicate rows in train: {train_dups}\")\n",
        "if train_dups > 0:\n",
        "    print(\"Dropping duplicate rows from train.\")\n",
        "    train_data = train_data.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Missing value summary\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "train_missing = train_data.isnull().sum()\n",
        "test_missing = test_data.isnull().sum()\n",
        "\n",
        "print(\"\\nMissing values in TRAIN:\")\n",
        "if train_missing.sum() == 0:\n",
        "    print(\"  No missing values!\")\n",
        "else:\n",
        "    print(train_missing[train_missing > 0])\n",
        "    print(f\"\\nTotal missing: {train_missing.sum()}\")\n",
        "    print(f\"Percentage: {(train_missing.sum() / (train_data.shape[0] * train_data.shape[1]) * 100):.2f}%\")\n",
        "\n",
        "print(\"\\nMissing values in TEST:\")\n",
        "if test_missing.sum() == 0:\n",
        "    print(\"  No missing values!\")\n",
        "else:\n",
        "    print(test_missing[test_missing > 0])\n",
        "    print(f\"\\nTotal missing: {test_missing.sum()}\")\n",
        "    print(f\"Percentage: {(test_missing.sum() / (test_data.shape[0] * test_data.shape[1]) * 100):.2f}%\")\n",
        "\n",
        "# Detect columns with excessive missing values\n",
        "high_missing_cols = train_missing[train_missing > len(train_data) * 0.5].index.tolist()\n",
        "if high_missing_cols:\n",
        "    print(f\"\\n⚠ WARNING: Columns with >50% missing values: {high_missing_cols}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. TARGET VARIABLE ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET VARIABLE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Keep a copy for visualization\n",
        "train_viz = train_data.copy()\n",
        "\n",
        "# Analyze each target column\n",
        "target_info = {}\n",
        "for target_col in target_cols:\n",
        "    print(f\"\\n{'-'*80}\")\n",
        "    print(f\"Target: {target_col}\")\n",
        "    print(f\"{'-'*80}\")\n",
        "\n",
        "    # Check for NaN in target\n",
        "    nan_count = train_data[target_col].isnull().sum()\n",
        "    if nan_count > 0:\n",
        "        print(f\"⚠ WARNING: {nan_count} NaN values in target '{target_col}'\")\n",
        "\n",
        "    # Get unique classes\n",
        "    unique_classes = train_data[target_col].dropna().unique()\n",
        "    n_classes = len(unique_classes)\n",
        "\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "    print(f\"Classes: {sorted(unique_classes)}\")\n",
        "\n",
        "    # Class distribution\n",
        "    value_counts = train_data[target_col].value_counts()\n",
        "    value_props = train_data[target_col].value_counts(normalize=True)\n",
        "\n",
        "    print(\"\\nClass distribution:\")\n",
        "    for cls in sorted(unique_classes):\n",
        "        count = value_counts.get(cls, 0)\n",
        "        prop = value_props.get(cls, 0)\n",
        "        print(f\"  {cls}: {count} ({prop*100:.2f}%)\")\n",
        "\n",
        "    # Check for severe imbalance\n",
        "    if n_classes >= 2:\n",
        "        min_prop = value_props.min()\n",
        "        if min_prop < 0.05:\n",
        "            print(f\"\\n⚠ Severe class imbalance detected! Minimum class: {min_prop*100:.2f}%\")\n",
        "\n",
        "    # Store info\n",
        "    target_info[target_col] = {\n",
        "        'n_classes': n_classes,\n",
        "        'classes': sorted(unique_classes),\n",
        "        'nan_count': nan_count\n",
        "    }\n",
        "\n",
        "# Plot target distributions\n",
        "if DO_PLOTTING:\n",
        "    n_targets = len(target_cols)\n",
        "    fig, axes = plt.subplots(1, n_targets, figsize=(6*n_targets, 5))\n",
        "    if n_targets == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, target_col in enumerate(target_cols):\n",
        "        train_viz[target_col].value_counts().plot(kind='bar', ax=axes[idx])\n",
        "        axes[idx].set_title(f\"Distribution of '{target_col}'\")\n",
        "        axes[idx].set_xlabel(\"Class\")\n",
        "        axes[idx].set_ylabel(\"Count\")\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. SEPARATE FEATURES AND TARGETS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE EXTRACTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Separate X and y\n",
        "X = train_data.drop(columns=target_cols)\n",
        "y = train_data[target_cols]\n",
        "\n",
        "# Handle NaN in target variables\n",
        "total_nan = y.isnull().any(axis=1).sum()\n",
        "if total_nan > 0:\n",
        "    print(f\"\\n⚠ Removing {total_nan} rows with NaN in target variable(s)\")\n",
        "    valid_idx = ~y.isnull().any(axis=1)\n",
        "    X = X[valid_idx].reset_index(drop=True)\n",
        "    y = y[valid_idx].reset_index(drop=True)\n",
        "    print(f\"New training shape: {X.shape}\")\n",
        "\n",
        "# For single target, convert to Series\n",
        "if not is_multi_target:\n",
        "    y = y.iloc[:, 0]\n",
        "\n",
        "# Feature type detection\n",
        "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "print(f\"\\nFeature summary:\")\n",
        "print(f\"  Total features: {X.shape[1]}\")\n",
        "print(f\"  Categorical: {len(cat_cols)}\")\n",
        "print(f\"  Numerical: {len(num_cols)}\")\n",
        "\n",
        "if cat_cols:\n",
        "    print(f\"\\nCategorical columns: {cat_cols}\")\n",
        "if num_cols:\n",
        "    print(f\"\\nNumerical columns: {num_cols}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 7. EXPLORATORY DATA ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "if DO_PLOTTING:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Numerical features - histograms\n",
        "    if len(num_cols) > 0:\n",
        "        print(\"\\nGenerating histograms for numerical features...\")\n",
        "        n_num = len(num_cols)\n",
        "        ncols = 3\n",
        "        nrows = (n_num + ncols - 1) // ncols\n",
        "        fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows))\n",
        "        axes = axes.flatten() if n_num > 1 else [axes]\n",
        "\n",
        "        for i, col in enumerate(num_cols):\n",
        "            sns.histplot(X[col].dropna(), kde=True, ax=axes[i])\n",
        "            axes[i].set_title(col)\n",
        "\n",
        "        # Hide empty subplots\n",
        "        for i in range(n_num, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Boxplots\n",
        "        print(\"\\nGenerating boxplots for numerical features...\")\n",
        "        fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows))\n",
        "        axes = axes.flatten() if n_num > 1 else [axes]\n",
        "\n",
        "        for i, col in enumerate(num_cols):\n",
        "            sns.boxplot(x=X[col].dropna(), ax=axes[i])\n",
        "            axes[i].set_title(f\"Boxplot: {col}\")\n",
        "\n",
        "        for i in range(n_num, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Correlation heatmap\n",
        "        if n_num > 1:\n",
        "            print(\"\\nGenerating correlation heatmap...\")\n",
        "            plt.figure(figsize=(12, 10))\n",
        "            corr = X[num_cols].corr()\n",
        "            sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
        "            plt.title(\"Correlation Matrix (Numerical Features)\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # Categorical features - count plots\n",
        "    if len(cat_cols) > 0:\n",
        "        print(\"\\nGenerating count plots for categorical features...\")\n",
        "        for col in cat_cols[:5]:  # Limit to first 5 to avoid too many plots\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            top_categories = X[col].value_counts().head(20)\n",
        "            sns.barplot(x=top_categories.values, y=top_categories.index)\n",
        "            plt.title(f\"Top 20 Categories in '{col}'\")\n",
        "            plt.xlabel(\"Count\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 8. OUTLIER DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OUTLIER ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "outlier_summary = {}\n",
        "for col in num_cols:\n",
        "    q1 = X[col].quantile(0.25)\n",
        "    q3 = X[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    n_lower = (X[col] < lower).sum()\n",
        "    n_upper = (X[col] > upper).sum()\n",
        "    outlier_summary[col] = {\n",
        "        \"lower\": lower,\n",
        "        \"upper\": upper,\n",
        "        \"n_lower\": int(n_lower),\n",
        "        \"n_upper\": int(n_upper)\n",
        "    }\n",
        "\n",
        "print(\"\\nOutlier summary (IQR method):\")\n",
        "for col, s in outlier_summary.items():\n",
        "    total_outliers = s['n_lower'] + s['n_upper']\n",
        "    pct = (total_outliers / len(X)) * 100\n",
        "    print(f\"  {col}: {s['n_lower']} below, {s['n_upper']} above ({pct:.2f}%)\")\n",
        "\n",
        "# Outlier capping\n",
        "if DO_OUTLIER_CAP:\n",
        "    print(\"\\n✓ Capping outliers using IQR thresholds...\")\n",
        "    for col, s in outlier_summary.items():\n",
        "        X[col] = X[col].clip(lower=s['lower'], upper=s['upper'])\n",
        "        if col in test_data.columns:\n",
        "            test_data[col] = test_data[col].clip(lower=s['lower'], upper=s['upper'])\n",
        "    print(\"Outlier capping completed.\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 9. MISSING VALUE IMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MISSING VALUE IMPUTATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Numerical imputation\n",
        "if num_cols:\n",
        "    print(\"\\n✓ Imputing numerical columns with MEDIAN...\")\n",
        "    num_imputer = SimpleImputer(strategy='median')\n",
        "    X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
        "    test_data[num_cols] = num_imputer.transform(test_data[num_cols])\n",
        "\n",
        "# Categorical imputation\n",
        "if cat_cols:\n",
        "    print(\"✓ Imputing categorical columns with MOST_FREQUENT...\")\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
        "    test_data[cat_cols] = cat_imputer.transform(test_data[cat_cols])\n",
        "\n",
        "# Verification\n",
        "train_missing_after = X.isnull().sum().sum()\n",
        "test_missing_after = test_data.isnull().sum().sum()\n",
        "\n",
        "print(f\"\\nPost-imputation verification:\")\n",
        "print(f\"  Train missing values: {train_missing_after}\")\n",
        "print(f\"  Test missing values: {test_missing_after}\")\n",
        "\n",
        "if train_missing_after > 0 or test_missing_after > 0:\n",
        "    print(\"\\n⚠ WARNING: Missing values still present!\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 10. PREPROCESSING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREPROCESSING PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
        "        ('num', StandardScaler(), num_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "print(\"✓ Preprocessing pipeline created\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 11. TRAIN/VALIDATION SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAIN/VALIDATION SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Stratify only for single-target classification\n",
        "stratify_param = y if not is_multi_target else None\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=stratify_param\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "\n",
        "if not is_multi_target:\n",
        "    print(\"\\nTrain set class distribution:\")\n",
        "    print(y_train.value_counts())\n",
        "    print(\"\\nValidation set class distribution:\")\n",
        "    print(y_val.value_counts())\n",
        "\n",
        "# Transform features\n",
        "print(\"\\n✓ Transforming features...\")\n",
        "X_train_pre = preprocessor.fit_transform(X_train)\n",
        "X_val_pre = preprocessor.transform(X_val)\n",
        "test_data_pre = preprocessor.transform(test_data)\n",
        "\n",
        "print(f\"  Train transformed: {X_train_pre.shape}\")\n",
        "print(f\"  Validation transformed: {X_val_pre.shape}\")\n",
        "print(f\"  Test transformed: {test_data_pre.shape}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 12. LABEL ENCODING (for single-target only)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LABEL ENCODING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not is_multi_target:\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(y_train)\n",
        "    y_val_enc = le.transform(y_val)\n",
        "\n",
        "    print(f\"Number of classes: {len(le.classes_)}\")\n",
        "    print(f\"Classes: {le.classes_}\")\n",
        "else:\n",
        "    # For multi-target, encode each target separately\n",
        "    label_encoders = {}\n",
        "    y_train_enc = pd.DataFrame(index=y_train.index)\n",
        "    y_val_enc = pd.DataFrame(index=y_val.index)\n",
        "\n",
        "    for target_col in target_cols:\n",
        "        le_temp = LabelEncoder()\n",
        "        y_train_enc[target_col] = le_temp.fit_transform(y_train[target_col])\n",
        "        y_val_enc[target_col] = le_temp.transform(y_val[target_col])\n",
        "        label_encoders[target_col] = le_temp\n",
        "\n",
        "        print(f\"\\nTarget '{target_col}':\")\n",
        "        print(f\"  Classes: {le_temp.classes_}\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    y_train_enc = y_train_enc.values\n",
        "    y_val_enc = y_val_enc.values\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 13. MODEL TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if is_multi_target:\n",
        "    print(\"\\n✓ Training Multi-Output RandomForest...\")\n",
        "    base_rf = RandomForestClassifier(\n",
        "        n_estimators=1000,\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight='balanced',\n",
        "        n_jobs=-1,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5\n",
        "    )\n",
        "    baseline_model = MultiOutputClassifier(base_rf, n_jobs=-1)\n",
        "else:\n",
        "    print(\"\\n✓ Training Single-Output RandomForest...\")\n",
        "    baseline_model = RandomForestClassifier(\n",
        "        n_estimators=1000,\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight='balanced',\n",
        "        n_jobs=-1,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5\n",
        "    )\n",
        "\n",
        "baseline_model.fit(X_train_pre, y_train_enc)\n",
        "print(\"✓ Baseline model training completed\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 14. MODEL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = baseline_model.predict(X_train_pre)\n",
        "y_val_pred = baseline_model.predict(X_val_pre)\n",
        "\n",
        "if is_multi_target:\n",
        "    # Multi-target evaluation\n",
        "    print(\"\\n✓ Multi-Target Classification Metrics:\")\n",
        "    print(f\"\\nTraining Set:\")\n",
        "    for idx, target_col in enumerate(target_cols):\n",
        "        acc = accuracy_score(y_train_enc[:, idx], y_train_pred[:, idx])\n",
        "        f1 = f1_score(y_train_enc[:, idx], y_train_pred[:, idx], average='macro')\n",
        "        print(f\"  {target_col}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "    print(f\"\\nValidation Set:\")\n",
        "    for idx, target_col in enumerate(target_cols):\n",
        "        acc = accuracy_score(y_val_enc[:, idx], y_val_pred[:, idx])\n",
        "        f1 = f1_score(y_val_enc[:, idx], y_val_pred[:, idx], average='macro')\n",
        "        print(f\"  {target_col}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "        # Detailed report for each target\n",
        "        print(f\"\\n  Classification Report for '{target_col}':\")\n",
        "        le_temp = label_encoders[target_col]\n",
        "        print(classification_report(y_val_enc[:, idx], y_val_pred[:, idx],\n",
        "                                   target_names=le_temp.classes_, zero_division=0))\n",
        "else:\n",
        "    # Single-target evaluation\n",
        "    n_classes = len(le.classes_)\n",
        "\n",
        "    # Get probabilities for ROC AUC\n",
        "    try:\n",
        "        train_proba = baseline_model.predict_proba(X_train_pre)\n",
        "        val_proba = baseline_model.predict_proba(X_val_pre)\n",
        "\n",
        "        if n_classes == 2:\n",
        "            roc_train = roc_auc_score(y_train_enc, train_proba[:, 1])\n",
        "            roc_val = roc_auc_score(y_val_enc, val_proba[:, 1])\n",
        "        else:\n",
        "            roc_train = roc_auc_score(y_train_enc, train_proba,\n",
        "                                     multi_class='ovr', average='macro')\n",
        "            roc_val = roc_auc_score(y_val_enc, val_proba,\n",
        "                                   multi_class='ovr', average='macro')\n",
        "\n",
        "        loss_train = log_loss(y_train_enc, train_proba)\n",
        "        loss_val = log_loss(y_val_enc, val_proba)\n",
        "\n",
        "        print(f\"\\nTraining Metrics:\")\n",
        "        print(f\"  ROC AUC: {roc_train:.4f}\")\n",
        "        print(f\"  Log Loss: {loss_train:.4f}\")\n",
        "\n",
        "        print(f\"\\nValidation Metrics:\")\n",
        "        print(f\"  ROC AUC: {roc_val:.4f}\")\n",
        "        print(f\"  Log Loss: {loss_val:.4f}\")\n",
        "    except:\n",
        "        print(\"\\nCould not compute probability-based metrics\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"Validation Classification Report:\")\n",
        "    print(\"-\"*80)\n",
        "    print(classification_report(y_val_enc, y_val_pred,\n",
        "                               target_names=le.classes_, zero_division=0))\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 15. HYPERPARAMETER TUNING (Optional)\n",
        "# ============================================================================\n",
        "\n",
        "tuned_model = None\n",
        "if DO_HYPERPARAM_TUNING:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"HYPERPARAMETER TUNING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    param_dist = {\n",
        "        \"estimator__n_estimators\" if is_multi_target else \"n_estimators\": [200, 500, 800, 1000],\n",
        "        \"estimator__max_depth\" if is_multi_target else \"max_depth\": [None, 10, 15, 20, 25],\n",
        "        \"estimator__min_samples_split\" if is_multi_target else \"min_samples_split\": [2, 5, 10],\n",
        "        \"estimator__min_samples_leaf\" if is_multi_target else \"min_samples_leaf\": [1, 2, 4],\n",
        "        \"estimator__max_features\" if is_multi_target else \"max_features\": [\"sqrt\", \"log2\", 0.3]\n",
        "    }\n",
        "\n",
        "    # Choose scoring\n",
        "    if is_multi_target:\n",
        "        scoring = 'accuracy'  # or custom scorer for multi-target\n",
        "    else:\n",
        "        scoring = 'roc_auc_ovr' if n_classes > 2 else 'roc_auc'\n",
        "\n",
        "    print(f\"\\n✓ Starting RandomizedSearchCV (n_iter=20)...\")\n",
        "\n",
        "    if is_multi_target:\n",
        "        base_estimator = MultiOutputClassifier(\n",
        "            RandomForestClassifier(random_state=RANDOM_STATE,\n",
        "                                 class_weight='balanced', n_jobs=-1),\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    else:\n",
        "        base_estimator = RandomForestClassifier(\n",
        "            random_state=RANDOM_STATE,\n",
        "            class_weight='balanced',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "    rnd_search = RandomizedSearchCV(\n",
        "        estimator=base_estimator,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=20,\n",
        "        scoring=scoring,\n",
        "        cv=3,\n",
        "        verbose=1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    rnd_search.fit(X_train_pre, y_train_enc)\n",
        "\n",
        "    print(\"\\n✓ Best parameters:\")\n",
        "    for param, value in rnd_search.best_params_.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "    print(f\"\\nBest CV score: {rnd_search.best_score_:.4f}\")\n",
        "\n",
        "    tuned_model = rnd_search.best_estimator_\n",
        "\n",
        "    # Evaluate tuned model\n",
        "    y_val_pred_tuned = tuned_model.predict(X_val_pre)\n",
        "\n",
        "    if is_multi_target:\n",
        "        print(\"\\nTuned Model - Validation Metrics:\")\n",
        "        for idx, target_col in enumerate(target_cols):\n",
        "            acc = accuracy_score(y_val_enc[:, idx], y_val_pred_tuned[:, idx])\n",
        "            f1 = f1_score(y_val_enc[:, idx], y_val_pred_tuned[:, idx], average='macro')\n",
        "            print(f\"  {target_col}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
        "    else:\n",
        "        try:\n",
        "            val_proba_tuned = tuned_model.predict_proba(X_val_pre)\n",
        "            if n_classes == 2:\n",
        "                roc_val_tuned = roc_auc_score(y_val_enc, val_proba_tuned[:, 1])\n",
        "            else:\n",
        "                roc_val_tuned = roc_auc_score(y_val_enc, val_proba_tuned,\n",
        "                                             multi_class='ovr', average='macro')\n",
        "            loss_val_tuned = log_loss(y_val_enc, val_proba_tuned)\n",
        "\n",
        "            print(f\"\\nTuned Model - Validation Metrics:\")\n",
        "            print(f\"  ROC AUC: {roc_val_tuned:.4f}\")\n",
        "            print(f\"  Log Loss: {loss_val_tuned:.4f}\")\n",
        "        except:\n",
        "            print(\"\\nCould not compute probability metrics for tuned model\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 16. CONFUSION MATRIX VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "if DO_PLOTTING and not is_multi_target:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CONFUSION MATRIX\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    final_model = tuned_model if tuned_model is not None else baseline_model\n",
        "    y_val_pred_final = final_model.predict(X_val_pre)\n",
        "\n",
        "    cm = confusion_matrix(y_val_enc, y_val_pred_final)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "    plt.title('Confusion Matrix - Validation Set')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 17. GENERATE PREDICTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING PREDICTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "final_model = tuned_model if tuned_model is not None else baseline_model\n",
        "print(f\"Using {'TUNED' if tuned_model is not None else 'BASELINE'} model\")\n",
        "\n",
        "# Make predictions\n",
        "test_pred_enc = final_model.predict(test_data_pre)\n",
        "\n",
        "# Decode predictions\n",
        "if is_multi_target:\n",
        "    test_pred_decoded = pd.DataFrame(index=range(len(test_pred_enc)))\n",
        "    for idx, target_col in enumerate(target_cols):\n",
        "        le_temp = label_encoders[target_col]\n",
        "        test_pred_decoded[target_col] = le_temp.inverse_transform(test_pred_enc[:, idx])\n",
        "else:\n",
        "    test_pred_decoded = le.inverse_transform(test_pred_enc)\n",
        "\n",
        "# Create submission\n",
        "if test_ids is not None:\n",
        "    if is_multi_target:\n",
        "        submission_df = pd.DataFrame({ID_COL: test_ids})\n",
        "        for target_col in target_cols:\n",
        "            submission_df[target_col] = test_pred_decoded[target_col].values\n",
        "    else:\n",
        "        submission_df = pd.DataFrame({\n",
        "            ID_COL: test_ids,\n",
        "            target_cols[0]: test_pred_decoded\n",
        "        })\n",
        "else:\n",
        "    if is_multi_target:\n",
        "        submission_df = test_pred_decoded.copy()\n",
        "    else:\n",
        "        submission_df = pd.DataFrame({target_cols[0]: test_pred_decoded})\n",
        "\n",
        "# Save submission\n",
        "submission_df.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"\\n✓ Submission saved to '{OUTPUT_FILE}'\")\n",
        "\n",
        "# Display statistics\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SUBMISSION STATISTICS\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Total predictions: {len(submission_df)}\")\n",
        "\n",
        "if is_multi_target:\n",
        "    for target_col in target_cols:\n",
        "        print(f\"\\nPrediction distribution for '{target_col}':\")\n",
        "        print(submission_df[target_col].value_counts())\n",
        "else:\n",
        "    print(\"\\nPrediction distribution:\")\n",
        "    print(submission_df[target_cols[0]].value_counts())\n",
        "\n",
        "print(\"\\nSubmission preview:\")\n",
        "print(submission_df.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ PROCESS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ]
}