{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt5HPE1tR41b"
      },
      "outputs": [],
      "source": [
        "# importing the useful libraries for the work\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import roc_auc_score, log_loss, classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. USER CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "TRAIN_PATH = \"/kaggle/input/mock-test-2-mse-2/train.csv\"\n",
        "TEST_PATH = \"/kaggle/input/mock-test-2-mse-2/test.csv\"\n",
        "\n",
        "TARGET_COL = \"Status\"\n",
        "ID_COL = \"id\"\n",
        "OUTPUT_FILE = \"submission.csv\"\n",
        "\n",
        "# Toggle heavy steps\n",
        "DO_PLOTTING = False  # Disabled for speed\n",
        "DO_OUTLIER_CAP = False\n",
        "DO_HYPERPARAM_TUNING = True\n",
        "DO_MODEL_COMPARISON = True\n",
        "DO_ENSEMBLE = True\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Speed optimization settings\n",
        "FAST_MODE = True  # Enable fast mode\n",
        "N_JOBS = -1  # Use all CPU cores\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_data = pd.read_csv(TRAIN_PATH)\n",
        "test_data = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(f\"\\nTrain shape: {train_data.shape}\")\n",
        "print(f\"Test shape: {test_data.shape}\")\n",
        "\n",
        "test_ids = test_data[ID_COL].copy() if ID_COL in test_data.columns else None\n",
        "\n",
        "if ID_COL in train_data.columns:\n",
        "    train_data = train_data.drop(columns=[ID_COL])\n",
        "if ID_COL in test_data.columns:\n",
        "    test_data = test_data.drop(columns=[ID_COL])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DETECT TARGET CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET CONFIGURATION DETECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if isinstance(TARGET_COL, str):\n",
        "    target_cols = [TARGET_COL]\n",
        "    is_multi_target = False\n",
        "    print(f\"\\n‚úì Single target mode: '{TARGET_COL}'\")\n",
        "elif isinstance(TARGET_COL, list):\n",
        "    target_cols = TARGET_COL\n",
        "    is_multi_target = True\n",
        "    print(f\"\\n‚úì Multi-target mode: {len(target_cols)} targets\")\n",
        "else:\n",
        "    raise ValueError(\"TARGET_COL must be a string or list of strings\")\n",
        "\n",
        "missing_targets = [col for col in target_cols if col not in train_data.columns]\n",
        "if missing_targets:\n",
        "    raise ValueError(f\"Target columns not found: {missing_targets}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. DATA CLEANING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA CLEANING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_dups = train_data.duplicated().sum()\n",
        "print(f\"\\nDuplicate rows in train: {train_dups}\")\n",
        "if train_dups > 0:\n",
        "    train_data = train_data.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "train_missing = train_data.isnull().sum()\n",
        "test_missing = test_data.isnull().sum()\n",
        "\n",
        "print(\"\\nMissing values in TRAIN:\")\n",
        "if train_missing.sum() == 0:\n",
        "    print(\"  No missing values!\")\n",
        "else:\n",
        "    print(train_missing[train_missing > 0])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. TARGET VARIABLE ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET VARIABLE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for target_col in target_cols:\n",
        "    print(f\"\\nTarget: {target_col}\")\n",
        "    unique_classes = train_data[target_col].dropna().unique()\n",
        "    print(f\"Number of classes: {len(unique_classes)}\")\n",
        "    print(f\"Classes: {sorted(unique_classes)}\")\n",
        "    print(\"\\nClass distribution:\")\n",
        "    print(train_data[target_col].value_counts())\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. SEPARATE FEATURES AND TARGETS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE EXTRACTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X = train_data.drop(columns=target_cols)\n",
        "y = train_data[target_cols]\n",
        "\n",
        "total_nan = y.isnull().any(axis=1).sum()\n",
        "if total_nan > 0:\n",
        "    valid_idx = ~y.isnull().any(axis=1)\n",
        "    X = X[valid_idx].reset_index(drop=True)\n",
        "    y = y[valid_idx].reset_index(drop=True)\n",
        "\n",
        "if not is_multi_target:\n",
        "    y = y.iloc[:, 0]\n",
        "\n",
        "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "print(f\"\\nFeature summary:\")\n",
        "print(f\"  Total features: {X.shape[1]}\")\n",
        "print(f\"  Categorical: {len(cat_cols)}\")\n",
        "print(f\"  Numerical: {len(num_cols)}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 7. OUTLIER DETECTION & HANDLING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OUTLIER ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "outlier_summary = {}\n",
        "for col in num_cols:\n",
        "    q1 = X[col].quantile(0.25)\n",
        "    q3 = X[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    n_lower = (X[col] < lower).sum()\n",
        "    n_upper = (X[col] > upper).sum()\n",
        "    outlier_summary[col] = {\"lower\": lower, \"upper\": upper, \"n_lower\": int(n_lower), \"n_upper\": int(n_upper)}\n",
        "\n",
        "if DO_OUTLIER_CAP:\n",
        "    print(\"\\n‚úì Capping outliers...\")\n",
        "    for col, s in outlier_summary.items():\n",
        "        X[col] = X[col].clip(lower=s['lower'], upper=s['upper'])\n",
        "        if col in test_data.columns:\n",
        "            test_data[col] = test_data[col].clip(lower=s['lower'], upper=s['upper'])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 8. MISSING VALUE IMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MISSING VALUE IMPUTATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if num_cols:\n",
        "    num_imputer = SimpleImputer(strategy='median')\n",
        "    X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
        "    test_data[num_cols] = num_imputer.transform(test_data[num_cols])\n",
        "\n",
        "if cat_cols:\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
        "    test_data[cat_cols] = cat_imputer.transform(test_data[cat_cols])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 9. PREPROCESSING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREPROCESSING PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
        "        ('num', StandardScaler(), num_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 10. TRAIN/VALIDATION SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAIN/VALIDATION SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "stratify_param = y if not is_multi_target else None\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.15, random_state=RANDOM_STATE, stratify=stratify_param  # Reduced to 15%\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "\n",
        "X_train_pre = preprocessor.fit_transform(X_train)\n",
        "X_val_pre = preprocessor.transform(X_val)\n",
        "test_data_pre = preprocessor.transform(test_data)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 11. LABEL ENCODING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LABEL ENCODING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not is_multi_target:\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(y_train)\n",
        "    y_val_enc = le.transform(y_val)\n",
        "    n_classes = len(le.classes_)\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "else:\n",
        "    label_encoders = {}\n",
        "    y_train_enc = pd.DataFrame(index=y_train.index)\n",
        "    y_val_enc = pd.DataFrame(index=y_val.index)\n",
        "    for target_col in target_cols:\n",
        "        le_temp = LabelEncoder()\n",
        "        y_train_enc[target_col] = le_temp.fit_transform(y_train[target_col])\n",
        "        y_val_enc[target_col] = le_temp.transform(y_val[target_col])\n",
        "        label_encoders[target_col] = le_temp\n",
        "    y_train_enc = y_train_enc.values\n",
        "    y_val_enc = y_val_enc.values\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 12. MODEL COMPARISON\n",
        "# ============================================================================\n",
        "\n",
        "if DO_MODEL_COMPARISON and not is_multi_target:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL COMPARISON - TESTING ALL CLASSIFIERS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Optimized models with reduced complexity for speed\n",
        "    models = {}\n",
        "\n",
        "    if FAST_MODE:\n",
        "        # Fast mode - only test top performers\n",
        "        models = {\n",
        "            'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
        "                                         random_state=RANDOM_STATE, eval_metric='logloss',\n",
        "                                         use_label_encoder=False, n_jobs=N_JOBS),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=15,\n",
        "                                                   random_state=RANDOM_STATE, n_jobs=N_JOBS),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5,\n",
        "                                                           random_state=RANDOM_STATE)\n",
        "        }\n",
        "    else:\n",
        "        # Full mode - test all models\n",
        "        models = {\n",
        "            'Logistic Regression': LogisticRegression(max_iter=500, random_state=RANDOM_STATE, n_jobs=N_JOBS),\n",
        "            'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=15),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=15, random_state=RANDOM_STATE, n_jobs=N_JOBS),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_STATE),\n",
        "            'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_STATE,\n",
        "                                         eval_metric='logloss', use_label_encoder=False, n_jobs=N_JOBS),\n",
        "            'KNN': KNeighborsClassifier(n_neighbors=5, n_jobs=N_JOBS),\n",
        "            'Naive Bayes': GaussianNB()\n",
        "        }\n",
        "        # Skip SVM in default mode as it's very slow\n",
        "        print(\"‚ö† Skipping SVM due to slow training time. Set FAST_MODE=False to include it.\")\n",
        "\n",
        "    model_results = {}\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\"{'Model':<25} {'Train Acc':<12} {'Val Acc':<12} {'Val F1':<12} {'Val ROC-AUC':<12}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            # Train\n",
        "            model.fit(X_train_pre, y_train_enc)\n",
        "\n",
        "            # Predictions\n",
        "            y_train_pred = model.predict(X_train_pre)\n",
        "            y_val_pred = model.predict(X_val_pre)\n",
        "\n",
        "            # Metrics\n",
        "            train_acc = accuracy_score(y_train_enc, y_train_pred)\n",
        "            val_acc = accuracy_score(y_val_enc, y_val_pred)\n",
        "            val_f1 = f1_score(y_val_enc, y_val_pred, average='macro')\n",
        "\n",
        "            # ROC AUC\n",
        "            try:\n",
        "                val_proba = model.predict_proba(X_val_pre)\n",
        "                if n_classes == 2:\n",
        "                    val_roc = roc_auc_score(y_val_enc, val_proba[:, 1])\n",
        "                else:\n",
        "                    val_roc = roc_auc_score(y_val_enc, val_proba, multi_class='ovr', average='macro')\n",
        "            except:\n",
        "                val_roc = 0.0\n",
        "\n",
        "            model_results[name] = {\n",
        "                'model': model,\n",
        "                'train_acc': train_acc,\n",
        "                'val_acc': val_acc,\n",
        "                'val_f1': val_f1,\n",
        "                'val_roc': val_roc\n",
        "            }\n",
        "\n",
        "            print(f\"{name:<25} {train_acc:<12.4f} {val_acc:<12.4f} {val_f1:<12.4f} {val_roc:<12.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{name:<25} ERROR: {str(e)[:40]}\")\n",
        "\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Find best model\n",
        "    best_model_name = max(model_results, key=lambda x: model_results[x]['val_roc'])\n",
        "    print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "    print(f\"   Validation ROC-AUC: {model_results[best_model_name]['val_roc']:.4f}\")\n",
        "    print(f\"   Validation Accuracy: {model_results[best_model_name]['val_acc']:.4f}\")\n",
        "    print(f\"   Validation F1: {model_results[best_model_name]['val_f1']:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 13. HYPERPARAMETER TUNING FOR BEST MODELS\n",
        "# ============================================================================\n",
        "\n",
        "tuned_models = {}\n",
        "\n",
        "if DO_HYPERPARAM_TUNING and not is_multi_target:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"HYPERPARAMETER TUNING - TOP MODELS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Sort models by performance\n",
        "    sorted_models = sorted(model_results.items(), key=lambda x: x[1]['val_roc'], reverse=True)\n",
        "\n",
        "    # Tune only top 2 models in fast mode, top 3 otherwise\n",
        "    num_to_tune = 2 if FAST_MODE else 3\n",
        "    top_models = sorted_models[:num_to_tune]\n",
        "\n",
        "    print(f\"\\nTuning top {num_to_tune} models: {[m[0] for m in top_models]}\")\n",
        "\n",
        "    # Reduced hyperparameter grids for speed\n",
        "    param_grids = {\n",
        "        'Random Forest': {\n",
        "            'n_estimators': [200, 400] if FAST_MODE else [300, 500, 800],\n",
        "            'max_depth': [15, 20, None] if FAST_MODE else [15, 20, 25, None],\n",
        "            'min_samples_split': [2, 5] if FAST_MODE else [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2] if FAST_MODE else [1, 2, 4],\n",
        "            'max_features': ['sqrt'] if FAST_MODE else ['sqrt', 'log2']\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'n_estimators': [200, 400] if FAST_MODE else [300, 500, 800],\n",
        "            'max_depth': [5, 7, 10] if FAST_MODE else [3, 5, 7, 10],\n",
        "            'learning_rate': [0.05, 0.1] if FAST_MODE else [0.01, 0.05, 0.1],\n",
        "            'subsample': [0.8, 1.0] if FAST_MODE else [0.8, 0.9, 1.0],\n",
        "            'colsample_bytree': [0.8, 1.0] if FAST_MODE else [0.8, 0.9, 1.0]\n",
        "        },\n",
        "        'Gradient Boosting': {\n",
        "            'n_estimators': [150, 300] if FAST_MODE else [200, 300, 500],\n",
        "            'max_depth': [3, 5] if FAST_MODE else [3, 5, 7],\n",
        "            'learning_rate': [0.05, 0.1] if FAST_MODE else [0.01, 0.05, 0.1],\n",
        "            'subsample': [0.8, 1.0] if FAST_MODE else [0.8, 0.9, 1.0]\n",
        "        },\n",
        "        'Logistic Regression': {\n",
        "            'C': [0.1, 1, 10] if FAST_MODE else [0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l2'],\n",
        "            'solver': ['lbfgs']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    scoring = 'roc_auc_ovr' if n_classes > 2 else 'roc_auc'\n",
        "    n_iter = 8 if FAST_MODE else 15\n",
        "    cv_folds = 3\n",
        "\n",
        "    for model_name, model_data in top_models:\n",
        "        if model_name in param_grids:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Tuning {model_name}...\")\n",
        "            print('='*60)\n",
        "\n",
        "            base_model = models[model_name]\n",
        "            param_grid = param_grids[model_name]\n",
        "\n",
        "            rnd_search = RandomizedSearchCV(\n",
        "                estimator=base_model,\n",
        "                param_distributions=param_grid,\n",
        "                n_iter=n_iter,\n",
        "                scoring=scoring,\n",
        "                cv=cv_folds,\n",
        "                verbose=0,  # Reduced verbosity\n",
        "                random_state=RANDOM_STATE,\n",
        "                n_jobs=N_JOBS\n",
        "            )\n",
        "\n",
        "            rnd_search.fit(X_train_pre, y_train_enc)\n",
        "\n",
        "            print(f\"\\n‚úì Best parameters for {model_name}:\")\n",
        "            for param, value in rnd_search.best_params_.items():\n",
        "                print(f\"  {param}: {value}\")\n",
        "            print(f\"Best CV score: {rnd_search.best_score_:.4f}\")\n",
        "\n",
        "            tuned_models[model_name] = rnd_search.best_estimator_\n",
        "\n",
        "            # Evaluate tuned model\n",
        "            y_val_pred_tuned = rnd_search.best_estimator_.predict(X_val_pre)\n",
        "            val_acc_tuned = accuracy_score(y_val_enc, y_val_pred_tuned)\n",
        "            val_f1_tuned = f1_score(y_val_enc, y_val_pred_tuned, average='macro')\n",
        "\n",
        "            try:\n",
        "                val_proba_tuned = rnd_search.best_estimator_.predict_proba(X_val_pre)\n",
        "                if n_classes == 2:\n",
        "                    val_roc_tuned = roc_auc_score(y_val_enc, val_proba_tuned[:, 1])\n",
        "                else:\n",
        "                    val_roc_tuned = roc_auc_score(y_val_enc, val_proba_tuned, multi_class='ovr', average='macro')\n",
        "                print(f\"\\nTuned {model_name} - Validation Metrics:\")\n",
        "                print(f\"  ROC-AUC: {val_roc_tuned:.4f}\")\n",
        "                print(f\"  Accuracy: {val_acc_tuned:.4f}\")\n",
        "                print(f\"  F1 Score: {val_f1_tuned:.4f}\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 14. ENSEMBLE METHODS\n",
        "# ============================================================================\n",
        "\n",
        "ensemble_model = None\n",
        "\n",
        "if DO_ENSEMBLE and not is_multi_target and len(tuned_models) > 0:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ENSEMBLE METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use only top 2 models for ensemble in fast mode\n",
        "    if FAST_MODE and len(tuned_models) > 2:\n",
        "        # Select top 2 tuned models\n",
        "        sorted_tuned = sorted(tuned_models.items(),\n",
        "                             key=lambda x: accuracy_score(y_val_enc, x[1].predict(X_val_pre)),\n",
        "                             reverse=True)[:2]\n",
        "        ensemble_estimators = sorted_tuned\n",
        "        print(f\"Fast mode: Using top 2 models for ensemble: {[m[0] for m in ensemble_estimators]}\")\n",
        "    else:\n",
        "        ensemble_estimators = list(tuned_models.items())\n",
        "\n",
        "    # Voting Classifier only (skip stacking in fast mode for speed)\n",
        "    print(\"\\n‚úì Creating Voting Classifier (Soft Voting)...\")\n",
        "\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=ensemble_estimators,\n",
        "        voting='soft',\n",
        "        n_jobs=N_JOBS\n",
        "    )\n",
        "\n",
        "    voting_clf.fit(X_train_pre, y_train_enc)\n",
        "\n",
        "    y_val_pred_voting = voting_clf.predict(X_val_pre)\n",
        "    val_acc_voting = accuracy_score(y_val_enc, y_val_pred_voting)\n",
        "    val_f1_voting = f1_score(y_val_enc, y_val_pred_voting, average='macro')\n",
        "\n",
        "    try:\n",
        "        val_proba_voting = voting_clf.predict_proba(X_val_pre)\n",
        "        if n_classes == 2:\n",
        "            val_roc_voting = roc_auc_score(y_val_enc, val_proba_voting[:, 1])\n",
        "        else:\n",
        "            val_roc_voting = roc_auc_score(y_val_enc, val_proba_voting, multi_class='ovr', average='macro')\n",
        "\n",
        "        print(f\"\\nVoting Classifier - Validation Metrics:\")\n",
        "        print(f\"  ROC-AUC: {val_roc_voting:.4f}\")\n",
        "        print(f\"  Accuracy: {val_acc_voting:.4f}\")\n",
        "        print(f\"  F1 Score: {val_f1_voting:.4f}\")\n",
        "\n",
        "        ensemble_model = voting_clf\n",
        "\n",
        "    except:\n",
        "        val_roc_voting = 0\n",
        "        ensemble_model = None\n",
        "\n",
        "    # Optionally create stacking classifier if not in fast mode\n",
        "    if not FAST_MODE:\n",
        "        print(\"\\n‚úì Creating Stacking Classifier...\")\n",
        "\n",
        "        stacking_clf = StackingClassifier(\n",
        "            estimators=ensemble_estimators,\n",
        "            final_estimator=LogisticRegression(max_iter=500),\n",
        "            cv=3,\n",
        "            n_jobs=N_JOBS\n",
        "        )\n",
        "\n",
        "        stacking_clf.fit(X_train_pre, y_train_enc)\n",
        "\n",
        "        y_val_pred_stacking = stacking_clf.predict(X_val_pre)\n",
        "        val_acc_stacking = accuracy_score(y_val_enc, y_val_pred_stacking)\n",
        "        val_f1_stacking = f1_score(y_val_enc, y_val_pred_stacking, average='macro')\n",
        "\n",
        "        try:\n",
        "            val_proba_stacking = stacking_clf.predict_proba(X_val_pre)\n",
        "            if n_classes == 2:\n",
        "                val_roc_stacking = roc_auc_score(y_val_enc, val_proba_stacking[:, 1])\n",
        "            else:\n",
        "                val_roc_stacking = roc_auc_score(y_val_enc, val_proba_stacking, multi_class='ovr', average='macro')\n",
        "\n",
        "            print(f\"\\nStacking Classifier - Validation Metrics:\")\n",
        "            print(f\"  ROC-AUC: {val_roc_stacking:.4f}\")\n",
        "            print(f\"  Accuracy: {val_acc_stacking:.4f}\")\n",
        "            print(f\"  F1 Score: {val_f1_stacking:.4f}\")\n",
        "        except:\n",
        "            val_roc_stacking = 0\n",
        "\n",
        "        # Choose best ensemble\n",
        "        if val_roc_stacking >= val_roc_voting:\n",
        "            ensemble_model = stacking_clf\n",
        "            print(f\"\\nüèÜ BEST ENSEMBLE: Stacking Classifier (ROC-AUC: {val_roc_stacking:.4f})\")\n",
        "        else:\n",
        "            ensemble_model = voting_clf\n",
        "            print(f\"\\nüèÜ BEST ENSEMBLE: Voting Classifier (ROC-AUC: {val_roc_voting:.4f})\")\n",
        "    else:\n",
        "        print(f\"\\nüèÜ Using Voting Classifier (ROC-AUC: {val_roc_voting:.4f})\")\n",
        "        print(\"‚ö† Skipping Stacking Classifier in FAST_MODE\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 15. FINAL MODEL SELECTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL MODEL SELECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not is_multi_target:\n",
        "    if ensemble_model is not None:\n",
        "        final_model = ensemble_model\n",
        "        print(\"Using ENSEMBLE model for predictions\")\n",
        "    elif tuned_models:\n",
        "        best_tuned = max(tuned_models.items(),\n",
        "                        key=lambda x: accuracy_score(y_val_enc, x[1].predict(X_val_pre)))\n",
        "        final_model = best_tuned[1]\n",
        "        print(f\"Using TUNED {best_tuned[0]} for predictions\")\n",
        "    else:\n",
        "        final_model = model_results[best_model_name]['model']\n",
        "        print(f\"Using {best_model_name} for predictions\")\n",
        "else:\n",
        "    # For multi-target, use Random Forest with MultiOutput\n",
        "    print(\"Using MultiOutput Random Forest for multi-target prediction\")\n",
        "    n_est = 300 if FAST_MODE else 500\n",
        "    final_model = MultiOutputClassifier(\n",
        "        RandomForestClassifier(n_estimators=n_est, max_depth=15, random_state=RANDOM_STATE, n_jobs=N_JOBS),\n",
        "        n_jobs=N_JOBS\n",
        "    )\n",
        "    final_model.fit(X_train_pre, y_train_enc)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 16. GENERATE PREDICTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING PREDICTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_pred_enc = final_model.predict(test_data_pre)\n",
        "\n",
        "if is_multi_target:\n",
        "    test_pred_decoded = pd.DataFrame(index=range(len(test_pred_enc)))\n",
        "    for idx, target_col in enumerate(target_cols):\n",
        "        le_temp = label_encoders[target_col]\n",
        "        test_pred_decoded[target_col] = le_temp.inverse_transform(test_pred_enc[:, idx])\n",
        "else:\n",
        "    test_pred_decoded = le.inverse_transform(test_pred_enc)\n",
        "\n",
        "if test_ids is not None:\n",
        "    if is_multi_target:\n",
        "        submission_df = pd.DataFrame({ID_COL: test_ids})\n",
        "        for target_col in target_cols:\n",
        "            submission_df[target_col] = test_pred_decoded[target_col].values\n",
        "    else:\n",
        "        submission_df = pd.DataFrame({\n",
        "            ID_COL: test_ids,\n",
        "            target_cols[0]: test_pred_decoded\n",
        "        })\n",
        "else:\n",
        "    if is_multi_target:\n",
        "        submission_df = test_pred_decoded.copy()\n",
        "    else:\n",
        "        submission_df = pd.DataFrame({target_cols[0]: test_pred_decoded})\n",
        "\n",
        "submission_df.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"\\n‚úì Submission saved to '{OUTPUT_FILE}'\")\n",
        "\n",
        "print(\"\\nSubmission preview:\")\n",
        "print(submission_df.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úì PROCESS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ]
}