{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2mG70SjM175"
      },
      "outputs": [],
      "source": [
        "# importing the useful libraries for the work\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, log_loss, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "# 1. USER CONFIGURATION\n",
        "\n",
        "TRAIN_PATH = \"/kaggle/input/mse-2-ai-201-b-ai-d/train.csv\"\n",
        "TEST_PATH = \"/kaggle/input/mse-2-ai-201-b-ai-d/test.csv\"\n",
        "TARGET_COL = \"Class\"\n",
        "ID_COL = \"id\"\n",
        "OUTPUT_FILE = \"submission.csv\"\n",
        "# Toggle heavy steps\n",
        "DO_PLOTTING = True\n",
        "DO_OUTLIER_CAP = False\n",
        "DO_HYPERPARAM_TUNING = True\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "\n",
        "# 2. Load Data\n",
        "print(\"Loading data...\")\n",
        "train_data = pd.read_csv(TRAIN_PATH)\n",
        "test_data = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(f\"Train shape: {train_data.shape}\")\n",
        "print(f\"Test shape: {test_data.shape}\")\n",
        "\n",
        "# Keep test IDs for submission\n",
        "test_ids = test_data[ID_COL].copy() if ID_COL in test_data.columns else None\n",
        "\n",
        "# Drop ID cols from feature tables\n",
        "if ID_COL in train_data.columns:\n",
        "    train_data = train_data.drop(columns=[ID_COL])\n",
        "if ID_COL in test_data.columns:\n",
        "    test_data = test_data.drop(columns=[ID_COL])\n",
        "\n",
        "\n",
        "# DATA CLEANING\n",
        "\n",
        "print(\"\\n=== Data Cleaning ===\")\n",
        "\n",
        "# 1) Basic info\n",
        "print(\"\\nTrain info:\")\n",
        "print(train_data.info())\n",
        "print(\"\\nTest info:\")\n",
        "print(test_data.info())\n",
        "\n",
        "# 2) Duplicates\n",
        "train_dups = train_data.duplicated().sum()\n",
        "print(f\"\\nDuplicate rows in train: {train_dups}\")\n",
        "if train_dups > 0:\n",
        "    print(\"Dropping duplicate rows from train.\")\n",
        "    train_data = train_data.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# 3) Missing value summary - ENHANCED\n",
        "print(\"\\n=== Missing Values Analysis ===\")\n",
        "train_missing = train_data.isnull().sum()\n",
        "test_missing = test_data.isnull().sum()\n",
        "\n",
        "print(\"\\nMissing values in TRAIN:\")\n",
        "print(train_missing[train_missing > 0])\n",
        "print(f\"\\nTotal missing values in train: {train_missing.sum()}\")\n",
        "print(f\"Percentage: {(train_missing.sum() / (train_data.shape[0] * train_data.shape[1]) * 100):.2f}%\")\n",
        "\n",
        "print(\"\\nMissing values in TEST:\")\n",
        "print(test_missing[test_missing > 0])\n",
        "print(f\"\\nTotal missing values in test: {test_missing.sum()}\")\n",
        "print(f\"Percentage: {(test_missing.sum() / (test_data.shape[0] * test_data.shape[1]) * 100):.2f}%\")\n",
        "\n",
        "# 4) Detect columns with excessive missing values (>50%)\n",
        "high_missing_cols = train_missing[train_missing > len(train_data) * 0.5].index.tolist()\n",
        "if high_missing_cols:\n",
        "    print(f\"\\nColumns with >50% missing values: {high_missing_cols}\")\n",
        "    print(\"Consider dropping these columns or using special imputation strategies.\")\n",
        "\n",
        "# 5) Inconsistent categorical values\n",
        "cat_columns_guess = train_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "# Remove target column from categorical check\n",
        "if TARGET_COL in cat_columns_guess:\n",
        "    cat_columns_guess.remove(TARGET_COL)\n",
        "\n",
        "print(f\"\\nDetected categorical columns: {cat_columns_guess}\")\n",
        "for c in cat_columns_guess:\n",
        "    unique_vals = train_data[c].dropna().unique()\n",
        "    if len(unique_vals) <= 20:\n",
        "        print(f\" Column `{c}` unique values: {unique_vals}\")\n",
        "    else:\n",
        "        print(f\" Column `{c}` has {len(unique_vals)} unique values\")\n",
        "\n",
        "# 6) Target distribution check - ENHANCED for multiclass\n",
        "if TARGET_COL in train_data.columns:\n",
        "    print(\"\\n=== Target Distribution (Multiclass) ===\")\n",
        "    target_counts = train_data[TARGET_COL].value_counts()\n",
        "    target_props = train_data[TARGET_COL].value_counts(normalize=True)\n",
        "\n",
        "    print(\"\\nClass counts:\")\n",
        "    print(target_counts)\n",
        "    print(\"\\nClass proportions:\")\n",
        "    print(target_props)\n",
        "\n",
        "    # Check for class imbalance\n",
        "    n_classes = len(target_counts)\n",
        "    print(f\"\\nNumber of classes: {n_classes}\")\n",
        "\n",
        "    if n_classes < 2:\n",
        "        raise ValueError(\"Target column has less than 2 classes. Cannot perform classification.\")\n",
        "\n",
        "    # Check for severe imbalance\n",
        "    min_class_prop = target_props.min()\n",
        "    if min_class_prop < 0.05:\n",
        "        print(f\"WARNING: Severe class imbalance detected. Minimum class proportion: {min_class_prop:.4f}\")\n",
        "        print(\"Using class_weight='balanced' to handle imbalance.\")\n",
        "else:\n",
        "    raise ValueError(f\"Target column {TARGET_COL} not found in train data.\")\n",
        "\n",
        "# Keep a copy for visualization\n",
        "train_viz = train_data.copy()\n",
        "\n",
        "\n",
        "# PREPROCESSING\n",
        "\n",
        "print(\"\\n=== Feature Engineering ===\")\n",
        "\n",
        "# Separate X and y - ENHANCED to handle NaN in target\n",
        "X = train_data.drop(columns=[TARGET_COL])\n",
        "y = train_data[TARGET_COL]\n",
        "\n",
        "# Check for NaN in target variable\n",
        "if y.isnull().sum() > 0:\n",
        "    print(f\"\\nWARNING: {y.isnull().sum()} NaN values found in target variable.\")\n",
        "    print(\"Dropping rows with NaN target values...\")\n",
        "    valid_idx = ~y.isnull()\n",
        "    X = X[valid_idx].reset_index(drop=True)\n",
        "    y = y[valid_idx].reset_index(drop=True)\n",
        "    print(f\"New training shape: {X.shape}\")\n",
        "\n",
        "# Dynamic feature detection\n",
        "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "print(f\"\\nDetected {len(cat_cols)} categorical columns: {cat_cols}\")\n",
        "print(f\"Detected {len(num_cols)} numerical columns: {num_cols}\")\n",
        "\n",
        "# Check data types of test to ensure consistency\n",
        "print(\"\\nVerifying test data columns match train...\")\n",
        "for col in X.columns:\n",
        "    if col in test_data.columns:\n",
        "        if X[col].dtype != test_data[col].dtype:\n",
        "            print(f\"WARNING: Column '{col}' has different dtypes: train={X[col].dtype}, test={test_data[col].dtype}\")\n",
        "\n",
        "\n",
        "# OUTLIER ANALYSIS & HANDLING\n",
        "\n",
        "print(\"\\n=== Outlier Analysis ===\")\n",
        "if DO_PLOTTING and len(num_cols) > 0:\n",
        "    # Histograms for numeric columns\n",
        "    n_num = len(num_cols)\n",
        "    ncols = 3\n",
        "    nrows = (n_num + ncols - 1) // ncols\n",
        "    plt.figure(figsize=(5 * ncols, 4 * nrows))\n",
        "    for i, col in enumerate(num_cols, 1):\n",
        "        plt.subplot(nrows, ncols, i)\n",
        "        sns.histplot(train_viz[col].dropna(), kde=True)\n",
        "        plt.title(col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Boxplots to visualize outliers\n",
        "    plt.figure(figsize=(5 * ncols, 4 * nrows))\n",
        "    for i, col in enumerate(num_cols, 1):\n",
        "        plt.subplot(nrows, ncols, i)\n",
        "        sns.boxplot(x=train_viz[col].dropna())\n",
        "        plt.title(f\"Boxplot: {col}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Correlation heatmap for numerical features\n",
        "    if n_num > 1:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        corr = train_viz[num_cols].corr()\n",
        "        sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "        plt.title(\"Correlation matrix (numeric features)\")\n",
        "        plt.show()\n",
        "\n",
        "    # Countplots for categorical features\n",
        "    for c in cat_cols:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        top_cats = train_viz[c].value_counts().head(20).index\n",
        "        sns.countplot(y=c, data=train_viz[train_viz[c].isin(top_cats)],\n",
        "                     order=top_cats)\n",
        "        plt.title(f\"Top 20 counts for {c}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Target distribution plot - ENHANCED for multiclass\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    train_viz[TARGET_COL].value_counts().plot(kind='bar')\n",
        "    plt.title(f\"Target Variable Distribution ({TARGET_COL})\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# IQR based outlier detection\n",
        "outlier_summary = {}\n",
        "for col in num_cols:\n",
        "    q1 = X[col].quantile(0.25)\n",
        "    q3 = X[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    n_lower = (X[col] < lower).sum()\n",
        "    n_upper = (X[col] > upper).sum()\n",
        "    outlier_summary[col] = {\n",
        "        \"lower\": lower,\n",
        "        \"upper\": upper,\n",
        "        \"n_lower\": int(n_lower),\n",
        "        \"n_upper\": int(n_upper)\n",
        "    }\n",
        "\n",
        "print(\"\\nOutlier summary (IQR method):\")\n",
        "for col, s in outlier_summary.items():\n",
        "    print(f\" {col}: {s['n_lower']} below threshold, {s['n_upper']} above threshold\")\n",
        "\n",
        "# Cap outliers if enabled\n",
        "if DO_OUTLIER_CAP:\n",
        "    print(\"\\nCapping numeric outliers using IQR thresholds...\")\n",
        "    for col, s in outlier_summary.items():\n",
        "        lower, upper = s['lower'], s['upper']\n",
        "        X[col] = X[col].clip(lower=lower, upper=upper)\n",
        "        if col in test_data.columns:\n",
        "            test_data[col] = test_data[col].clip(lower=lower, upper=upper)\n",
        "    print(\"Outlier capping completed.\")\n",
        "\n",
        "\n",
        "# MISSING VALUES IMPUTATION - ENHANCED\n",
        "\n",
        "print(\"\\n=== Advanced Missing Value Imputation ===\")\n",
        "\n",
        "# Strategy 1: For numeric columns - use median (more robust than mean)\n",
        "if num_cols:\n",
        "    print(\"\\nImputing numeric columns with MEDIAN (robust to outliers)...\")\n",
        "    num_imputer = SimpleImputer(strategy='median')\n",
        "    X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
        "    test_data[num_cols] = num_imputer.transform(test_data[num_cols])\n",
        "    print(\"Numeric imputation completed.\")\n",
        "\n",
        "# Strategy 2: For categorical columns - use most frequent (mode)\n",
        "if cat_cols:\n",
        "    print(\"\\nImputing categorical columns with MOST_FREQUENT...\")\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
        "    test_data[cat_cols] = cat_imputer.transform(test_data[cat_cols])\n",
        "    print(\"Categorical imputation completed.\")\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"\\n=== Post-Imputation Verification ===\")\n",
        "train_missing_after = X.isnull().sum().sum()\n",
        "test_missing_after = test_data.isnull().sum().sum()\n",
        "\n",
        "print(f\"Missing values in train after imputation: {train_missing_after}\")\n",
        "print(f\"Missing values in test after imputation: {test_missing_after}\")\n",
        "\n",
        "if train_missing_after > 0 or test_missing_after > 0:\n",
        "    print(\"\\nWARNING: Some missing values still remain!\")\n",
        "    print(\"\\nTrain columns with missing values:\")\n",
        "    print(X.isnull().sum()[X.isnull().sum() > 0])\n",
        "    print(\"\\nTest columns with missing values:\")\n",
        "    print(test_data.isnull().sum()[test_data.isnull().sum() > 0])\n",
        "\n",
        "\n",
        "# DEFINE PREPROCESSOR - ENHANCED\n",
        "\n",
        "print(\"\\n=== Building Preprocessing Pipeline ===\")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
        "        ('num', StandardScaler(), num_cols)\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep any other columns as-is\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline created.\")\n",
        "\n",
        "\n",
        "# TRAIN/VALIDATION SPLIT - STRATIFIED for multiclass\n",
        "\n",
        "print(\"\\n=== Creating Train/Validation Split ===\")\n",
        "\n",
        "# Use stratified split to maintain class distribution\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y  # IMPORTANT for multiclass\n",
        ")\n",
        "\n",
        "print(f\"Train set size: {X_train.shape[0]}\")\n",
        "print(f\"Validation set size: {X_val.shape[0]}\")\n",
        "print(\"\\nTrain set class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nValidation set class distribution:\")\n",
        "print(y_val.value_counts())\n",
        "\n",
        "# Fit preprocessor\n",
        "print(\"\\nTransforming data (fitting preprocessor on train)...\")\n",
        "X_train_pre = preprocessor.fit_transform(X_train)\n",
        "X_val_pre = preprocessor.transform(X_val)\n",
        "test_data_pre = preprocessor.transform(test_data)\n",
        "\n",
        "print(f\"Transformed train shape: {X_train_pre.shape}\")\n",
        "print(f\"Transformed validation shape: {X_val_pre.shape}\")\n",
        "print(f\"Transformed test shape: {test_data_pre.shape}\")\n",
        "\n",
        "\n",
        "# LABEL ENCODING - ENHANCED for multiclass\n",
        "\n",
        "print(\"\\n=== Label Encoding Target Variable ===\")\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_val_enc = le.transform(y_val)\n",
        "\n",
        "print(f\"Number of unique classes: {len(le.classes_)}\")\n",
        "print(f\"Class labels: {le.classes_}\")\n",
        "print(f\"Encoded values: {np.unique(y_train_enc)}\")\n",
        "\n",
        "\n",
        "# BASELINE MODEL - ENHANCED for multiclass\n",
        "\n",
        "print(\"\\n=== Baseline RandomForest Training (Multiclass) ===\")\n",
        "\n",
        "baseline_rfc = RandomForestClassifier(\n",
        "    n_estimators=1000,\n",
        "    random_state=RANDOM_STATE,\n",
        "    class_weight='balanced',  # Handle class imbalance\n",
        "    n_jobs=-1,\n",
        "    max_depth=20,  # Prevent overfitting\n",
        "    min_samples_split=5\n",
        ")\n",
        "\n",
        "print(\"Training baseline model...\")\n",
        "baseline_rfc.fit(X_train_pre, y_train_enc)\n",
        "print(\"Baseline model training completed.\")\n",
        "\n",
        "# Baseline predictions\n",
        "train_proba_base = baseline_rfc.predict_proba(X_train_pre)\n",
        "val_proba_base = baseline_rfc.predict_proba(X_val_pre)\n",
        "\n",
        "# Metrics for baseline - ENHANCED for multiclass\n",
        "n_classes_base = len(le.classes_)\n",
        "print(f\"\\nNumber of classes: {n_classes_base}\")\n",
        "\n",
        "if n_classes_base == 2:\n",
        "    # Binary classification\n",
        "    roc_train_base = roc_auc_score(y_train_enc, train_proba_base[:, 1])\n",
        "    roc_val_base = roc_auc_score(y_val_enc, val_proba_base[:, 1])\n",
        "else:\n",
        "    # Multiclass classification - use 'ovr' (one-vs-rest)\n",
        "    roc_train_base = roc_auc_score(y_train_enc, train_proba_base,\n",
        "                                   multi_class='ovr', average='macro')\n",
        "    roc_val_base = roc_auc_score(y_val_enc, val_proba_base,\n",
        "                                 multi_class='ovr', average='macro')\n",
        "\n",
        "loss_train_base = log_loss(y_train_enc, train_proba_base)\n",
        "loss_val_base = log_loss(y_val_enc, val_proba_base)\n",
        "\n",
        "print(\"\\n=== Baseline Model Performance ===\")\n",
        "print(f\"Training ROC AUC (macro): {roc_train_base:.4f}\")\n",
        "print(f\"Validation ROC AUC (macro): {roc_val_base:.4f}\")\n",
        "print(f\"Training Log Loss: {loss_train_base:.4f}\")\n",
        "print(f\"Validation Log Loss: {loss_val_base:.4f}\")\n",
        "\n",
        "# Classification report for validation set\n",
        "y_val_pred = baseline_rfc.predict(X_val_pre)\n",
        "print(\"\\nValidation Classification Report:\")\n",
        "print(classification_report(y_val_enc, y_val_pred, target_names=le.classes_))\n",
        "\n",
        "\n",
        "# HYPERPARAMETER TUNING - ENHANCED for multiclass\n",
        "\n",
        "tuned_rfc = None\n",
        "if DO_HYPERPARAM_TUNING:\n",
        "    print(\"\\n=== Hyperparameter Tuning (RandomizedSearchCV) ===\")\n",
        "\n",
        "    # Enhanced parameter grid for multiclass\n",
        "    param_dist = {\n",
        "        \"n_estimators\": [200, 500, 800, 1000, 1500],\n",
        "        \"max_depth\": [None, 10, 15, 20, 25],\n",
        "        \"min_samples_split\": [2, 5, 10],\n",
        "        \"min_samples_leaf\": [1, 2, 4],\n",
        "        \"max_features\": [\"sqrt\", \"log2\", 0.3, 0.5],\n",
        "        \"criterion\": [\"gini\", \"entropy\"]\n",
        "    }\n",
        "\n",
        "    # Choose appropriate scoring metric\n",
        "    scoring = 'roc_auc_ovr' if n_classes_base > 2 else 'roc_auc'\n",
        "\n",
        "    rnd_search = RandomizedSearchCV(\n",
        "        estimator=RandomForestClassifier(\n",
        "            random_state=RANDOM_STATE,\n",
        "            class_weight='balanced',\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=30,  # Increased for better search\n",
        "        scoring=scoring,\n",
        "        cv=5,  # 5-fold CV for better estimate\n",
        "        verbose=2,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    print(\"Starting hyperparameter search (this may take a while)...\")\n",
        "    rnd_search.fit(X_train_pre, y_train_enc)\n",
        "\n",
        "    print(\"\\n=== Tuning Results ===\")\n",
        "    print(\"Best parameters found:\")\n",
        "    for param, value in rnd_search.best_params_.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "    print(f\"\\nBest CV score: {rnd_search.best_score_:.4f}\")\n",
        "\n",
        "    tuned_rfc = rnd_search.best_estimator_\n",
        "\n",
        "    # Evaluate tuned model\n",
        "    train_proba_tuned = tuned_rfc.predict_proba(X_train_pre)\n",
        "    val_proba_tuned = tuned_rfc.predict_proba(X_val_pre)\n",
        "\n",
        "    if n_classes_base == 2:\n",
        "        roc_train_tuned = roc_auc_score(y_train_enc, train_proba_tuned[:, 1])\n",
        "        roc_val_tuned = roc_auc_score(y_val_enc, val_proba_tuned[:, 1])\n",
        "    else:\n",
        "        roc_train_tuned = roc_auc_score(y_train_enc, train_proba_tuned,\n",
        "                                       multi_class='ovr', average='macro')\n",
        "        roc_val_tuned = roc_auc_score(y_val_enc, val_proba_tuned,\n",
        "                                     multi_class='ovr', average='macro')\n",
        "\n",
        "    loss_train_tuned = log_loss(y_train_enc, train_proba_tuned)\n",
        "    loss_val_tuned = log_loss(y_val_enc, val_proba_tuned)\n",
        "\n",
        "    print(\"\\n=== Tuned Model Performance ===\")\n",
        "    print(f\"Training ROC AUC (macro): {roc_train_tuned:.4f}\")\n",
        "    print(f\"Validation ROC AUC (macro): {roc_val_tuned:.4f}\")\n",
        "    print(f\"Training Log Loss: {loss_train_tuned:.4f}\")\n",
        "    print(f\"Validation Log Loss: {loss_val_tuned:.4f}\")\n",
        "\n",
        "    # Classification report for tuned model\n",
        "    y_val_pred_tuned = tuned_rfc.predict(X_val_pre)\n",
        "    print(\"\\nTuned Model - Validation Classification Report:\")\n",
        "    print(classification_report(y_val_enc, y_val_pred_tuned, target_names=le.classes_))\n",
        "\n",
        "    # Improvement comparison\n",
        "    print(\"\\n=== Improvement Summary ===\")\n",
        "    print(f\"ROC AUC improvement: {(roc_val_tuned - roc_val_base):.4f}\")\n",
        "    print(f\"Log Loss improvement: {(loss_val_base - loss_val_tuned):.4f}\")\n",
        "else:\n",
        "    print(\"\\nHyperparameter tuning skipped by configuration.\")\n",
        "\n",
        "\n",
        "# SELECT MODEL FOR SUBMISSION\n",
        "\n",
        "use_tuned = True if (DO_HYPERPARAM_TUNING and tuned_rfc is not None) else False\n",
        "model_for_submission = tuned_rfc if use_tuned else baseline_rfc\n",
        "\n",
        "if use_tuned:\n",
        "    print(\"\\n✓ Using TUNED model for final predictions.\")\n",
        "else:\n",
        "    print(\"\\n✓ Using BASELINE model for final predictions.\")\n",
        "\n",
        "\n",
        "# FEATURE IMPORTANCE ANALYSIS\n",
        "\n",
        "print(\"\\n=== Feature Importance Analysis ===\")\n",
        "\n",
        "# Get feature names safely\n",
        "try:\n",
        "    if len(cat_cols) > 0:\n",
        "        cat_feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols))\n",
        "    else:\n",
        "        cat_feature_names = []\n",
        "\n",
        "    feature_names = cat_feature_names + num_cols\n",
        "except:\n",
        "    # Fallback if get_feature_names_out fails\n",
        "    print(\"Using simplified feature names...\")\n",
        "    feature_names = [f\"cat_feature_{i}\" for i in range(len(cat_cols) * 10)] + num_cols\n",
        "    feature_names = feature_names[:len(model_for_submission.feature_importances_)]\n",
        "\n",
        "importances = model_for_submission.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "print(\"\\nTop 20 most important features:\")\n",
        "for i in range(min(20, len(feature_names))):\n",
        "    print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "\n",
        "if DO_PLOTTING and len(feature_names) > 0:\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    top_n = min(20, len(feature_names))\n",
        "    plt.barh(range(top_n), importances[indices[:top_n]][::-1])\n",
        "    plt.yticks(range(top_n), [feature_names[i] for i in indices[:top_n]][::-1])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title('Top 20 Feature Importances')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# CONFUSION MATRIX VISUALIZATION\n",
        "\n",
        "if DO_PLOTTING:\n",
        "    print(\"\\n=== Confusion Matrix ===\")\n",
        "    y_val_pred_final = model_for_submission.predict(X_val_pre)\n",
        "    cm = confusion_matrix(y_val_enc, y_val_pred_final)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "    plt.title('Confusion Matrix - Validation Set')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# GENERATE SUBMISSION - ENHANCED\n",
        "\n",
        "print(\"\\n=== Generating Final Submission ===\")\n",
        "\n",
        "# Get predictions\n",
        "test_pred_enc = model_for_submission.predict(test_data_pre)\n",
        "test_pred = le.inverse_transform(test_pred_enc)\n",
        "\n",
        "# Get prediction probabilities for analysis\n",
        "test_pred_proba = model_for_submission.predict_proba(test_data_pre)\n",
        "\n",
        "# Create submission dataframe\n",
        "if test_ids is not None:\n",
        "    submission_df = pd.DataFrame({\n",
        "        ID_COL: test_ids,\n",
        "        TARGET_COL: test_pred\n",
        "    })\n",
        "else:\n",
        "    submission_df = pd.DataFrame({TARGET_COL: test_pred})\n",
        "\n",
        "# Save submission\n",
        "submission_df.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"\\n✓ Submission saved to {OUTPUT_FILE}\")\n",
        "\n",
        "# Display submission statistics\n",
        "print(\"\\n=== Submission Statistics ===\")\n",
        "print(f\"Total predictions: {len(submission_df)}\")\n",
        "print(\"\\nPredicted class distribution:\")\n",
        "print(submission_df[TARGET_COL].value_counts())\n",
        "print(\"\\nPredicted class proportions:\")\n",
        "print(submission_df[TARGET_COL].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nSubmission preview:\")\n",
        "print(submission_df.head(10))\n",
        "\n",
        "# Prediction confidence analysis\n",
        "max_proba = test_pred_proba.max(axis=1)\n",
        "print(f\"\\nPrediction confidence statistics:\")\n",
        "print(f\"Mean confidence: {max_proba.mean():.4f}\")\n",
        "print(f\"Median confidence: {np.median(max_proba):.4f}\")\n",
        "print(f\"Min confidence: {max_proba.min():.4f}\")\n",
        "print(f\"Max confidence: {max_proba.max():.4f}\")\n",
        "\n",
        "if DO_PLOTTING:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(max_proba, bins=50, edgecolor='black')\n",
        "    plt.xlabel('Prediction Confidence (Max Probability)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Prediction Confidence')\n",
        "    plt.axvline(max_proba.mean(), color='r', linestyle='--',\n",
        "                label=f'Mean: {max_proba.mean():.3f}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ SUBMISSION COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}